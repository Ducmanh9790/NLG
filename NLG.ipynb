{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPuOYLFv3+NuE/cLyxiDGnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ducmanh9790/NLG/blob/main/NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EbCsHckNKxV",
        "outputId": "67a4a0dc-257a-40b0-8263-2d41eb5a65e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LoRA'...\n",
            "remote: Enumerating objects: 2024, done.\u001b[K\n",
            "remote: Counting objects: 100% (770/770), done.\u001b[K\n",
            "remote: Compressing objects: 100% (413/413), done.\u001b[K\n",
            "remote: Total 2024 (delta 517), reused 357 (delta 357), pack-reused 1254 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2024/2024), 34.72 MiB | 30.86 MiB/s, done.\n",
            "Resolving deltas: 100% (814/814), done.\n",
            "/content/LoRA\n",
            "Obtaining file:///content/LoRA\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: loralib\n",
            "  Running setup.py develop for loralib\n",
            "Successfully installed loralib-0.1.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting progress\n",
            "  Downloading progress-1.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Downloading progress-1.6.1-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: progress\n",
            "Successfully installed progress-1.6.1\n",
            "✓ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Clone repo & install\n",
        "!git clone https://github.com/microsoft/LoRA.git\n",
        "%cd LoRA\n",
        "\n",
        "# Cài loralib\n",
        "!pip install -e .\n",
        "\n",
        "# Cài dependencies NLG\n",
        "!pip install transformers tqdm tensorboard progress numpy pyyaml\n",
        "\n",
        "print(\"✓ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download GPT-2 Medium\n",
        "import os\n",
        "os.makedirs(\"examples/NLG/pretrained_checkpoints\", exist_ok=True)\n",
        "\n",
        "!wget -O examples/NLG/pretrained_checkpoints/gpt2-medium-pytorch_model.bin \\\n",
        "  https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin\n",
        "\n",
        "print(\"✓ GPT-2 Medium downloaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whhEw4kvN3aO",
        "outputId": "c05e3b47-aa32-4110-b53a-841977be3cae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-04 07:16:02--  https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.170.185.14, 3.170.185.33, 3.170.185.25, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.170.185.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc036468d709f17434b/6056cd022b933c356a256889bf854d6273e618fe8c5dd6980439c81c324ffa4a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251204T071602Z&X-Amz-Expires=3600&X-Amz-Signature=d2f3bfda6a3fb3e80135643e8c84cf7cc5e9acdd35fb984da705355e11b4be4e&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&x-id=GetObject&Expires=1764836162&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NDgzNjE2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMDM2NDY4ZDcwOWYxNzQzNGIvNjA1NmNkMDIyYjkzM2MzNTZhMjU2ODg5YmY4NTRkNjI3M2U2MThmZThjNWRkNjk4MDQzOWM4MWMzMjRmZmE0YSoifV19&Signature=tFHox0Iy6bFNKnJ06erfxXKNPz9dL9wvAbVeO8BGU9cbQsZ84n2et40PFSkoCB6foGWZxphCMmqWk1YdpMq9LijgCB%7EJbIAWZZOJk7vgYzZ61ot6XWhQ9qM%7EokvzUxg8-2RKf1gFFN3H9axRg8fS1IlwrTNz8XBzMqRcnz2ztwy5tCn-gy97hnZkBVEjhVRMUgF9r7LxdohG7b6P66JpcGc8z5kTjAys1AE71Fek5Sf3o30lN7fRBzumQTc1kk5BZMm1ulotHU7LRqqufV3hTkvLaBgnuDv4w5gkpvwAUhtWVW0cJDKkRXBYQ4PJ3ZoO9c9cv3qwfEXFcyiBMukJkA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-04 07:16:02--  https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc036468d709f17434b/6056cd022b933c356a256889bf854d6273e618fe8c5dd6980439c81c324ffa4a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251204%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251204T071602Z&X-Amz-Expires=3600&X-Amz-Signature=d2f3bfda6a3fb3e80135643e8c84cf7cc5e9acdd35fb984da705355e11b4be4e&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&x-id=GetObject&Expires=1764836162&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NDgzNjE2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMDM2NDY4ZDcwOWYxNzQzNGIvNjA1NmNkMDIyYjkzM2MzNTZhMjU2ODg5YmY4NTRkNjI3M2U2MThmZThjNWRkNjk4MDQzOWM4MWMzMjRmZmE0YSoifV19&Signature=tFHox0Iy6bFNKnJ06erfxXKNPz9dL9wvAbVeO8BGU9cbQsZ84n2et40PFSkoCB6foGWZxphCMmqWk1YdpMq9LijgCB%7EJbIAWZZOJk7vgYzZ61ot6XWhQ9qM%7EokvzUxg8-2RKf1gFFN3H9axRg8fS1IlwrTNz8XBzMqRcnz2ztwy5tCn-gy97hnZkBVEjhVRMUgF9r7LxdohG7b6P66JpcGc8z5kTjAys1AE71Fek5Sf3o30lN7fRBzumQTc1kk5BZMm1ulotHU7LRqqufV3hTkvLaBgnuDv4w5gkpvwAUhtWVW0cJDKkRXBYQ4PJ3ZoO9c9cv3qwfEXFcyiBMukJkA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.252.122, 13.33.252.50, 13.33.252.46, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.252.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520013706 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘examples/NLG/pretrained_checkpoints/gpt2-medium-pytorch_model.bin’\n",
            "\n",
            "examples/NLG/pretra 100%[===================>]   1.42G   118MB/s    in 11s     \n",
            "\n",
            "2025-12-04 07:16:14 (130 MB/s) - ‘examples/NLG/pretrained_checkpoints/gpt2-medium-pytorch_model.bin’ saved [1520013706/1520013706]\n",
            "\n",
            "✓ GPT-2 Medium downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Format E2E data\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "os.chdir(\"examples/NLG\")\n",
        "\n",
        "# Format train\n",
        "!python src/format_converting_e2e.py data/e2e/train.txt data/e2e/train_formatted.jsonl\n",
        "!python src/gpt2_encode.py --vocab vocab --input data/e2e/train_formatted.jsonl \\\n",
        "  --output data/e2e/train.jsonl --add_bos --add_eos\n",
        "\n",
        "# Format valid\n",
        "!python src/format_converting_e2e.py data/e2e/valid.txt data/e2e/valid_formatted.jsonl\n",
        "!python src/gpt2_encode.py --vocab vocab --input data/e2e/valid_formatted.jsonl \\\n",
        "  --output data/e2e/valid.jsonl --add_bos --add_eos\n",
        "\n",
        "# Format test\n",
        "!python src/format_converting_e2e.py data/e2e/test.txt data/e2e/test_formatted.jsonl\n",
        "!python src/gpt2_encode.py --vocab vocab --input data/e2e/test_formatted.jsonl \\\n",
        "  --output data/e2e/test.jsonl --add_bos --add_eos\n",
        "\n",
        "print(\"✓ E2E datasets formatted!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxakyCfuN9zL",
        "outputId": "2e5d36dd-bdd3-4c69-e026-adbe7dafb8c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ E2E datasets formatted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Patch gpu.py cho Colab (single GPU mode)\n",
        "import os\n",
        "\n",
        "gpu_py_path = \"/content/LoRA/examples/NLG/src/gpu.py\"\n",
        "\n",
        "# Read current file\n",
        "with open(gpu_py_path, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Replace parse_gpu function\n",
        "old_parse_gpu = \"\"\"def parse_gpu(args):\n",
        "    torch.manual_seed(args.random_seed)\n",
        "\n",
        "    if args.platform == 'local':\n",
        "        dist.init_process_group(backend='nccl')\n",
        "        local_rank = torch.distributed.get_rank()\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        device = torch.device('cuda', local_rank)\n",
        "        args.rank = local_rank\n",
        "        args.device = device\n",
        "        args.world_size = torch.distributed.get_world_size()\n",
        "        args.dist = dist\"\"\"\n",
        "\n",
        "new_parse_gpu = \"\"\"def parse_gpu(args):\n",
        "    torch.manual_seed(args.random_seed)\n",
        "\n",
        "    if args.platform == 'local':\n",
        "        # Single GPU mode - no distributed training\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device('cuda', 0)\n",
        "            torch.cuda.set_device(0)\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "        args.rank = 0\n",
        "        args.device = device\n",
        "        args.world_size = 1\n",
        "        args.local_rank = 0\n",
        "        args.dist = None\"\"\"\n",
        "\n",
        "content = content.replace(old_parse_gpu, new_parse_gpu)\n",
        "\n",
        "# Write back\n",
        "with open(gpu_py_path, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"✓ gpu.py patched for Colab!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doZGy6X8OB2N",
        "outputId": "0800369b-38a2-4780-f2ab-6dae302003ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ gpu.py patched for Colab!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Patch gpt2_ft.py cho Colab\n",
        "import os\n",
        "\n",
        "gpt2_ft_path = \"/content/LoRA/examples/NLG/src/gpt2_ft.py\"\n",
        "\n",
        "with open(gpt2_ft_path, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Fix DistributedSampler\n",
        "old_sampler = \"\"\"    train_loader = DataLoader(\n",
        "        train_data, batch_size=args.train_batch_size, num_workers=0,\n",
        "        shuffle=False, pin_memory=False, drop_last=True,\n",
        "        sampler=torch.utils.data.distributed.DistributedSampler(train_data, seed=args.random_seed)\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        valid_data, batch_size=args.valid_batch_size, num_workers=0,\n",
        "        shuffle=False, pin_memory=False, drop_last=False,\n",
        "        sampler=torch.utils.data.distributed.DistributedSampler(valid_data, seed=args.random_seed)\n",
        "    )\"\"\"\n",
        "\n",
        "new_sampler = \"\"\"    # Create sampler only for distributed training\n",
        "    train_sampler = None\n",
        "    valid_sampler = None\n",
        "    if args.world_size > 1:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data, seed=args.random_seed)\n",
        "        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_data, seed=args.random_seed)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data, batch_size=args.train_batch_size, num_workers=0,\n",
        "        shuffle=(train_sampler is None), pin_memory=False, drop_last=True,\n",
        "        sampler=train_sampler\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        valid_data, batch_size=args.valid_batch_size, num_workers=0,\n",
        "        shuffle=False, pin_memory=False, drop_last=False,\n",
        "        sampler=valid_sampler\n",
        "    )\"\"\"\n",
        "\n",
        "content = content.replace(old_sampler, new_sampler)\n",
        "\n",
        "# Fix set_epoch\n",
        "old_set_epoch = \"    train_loader.sampler.set_epoch(epoch)\"\n",
        "new_set_epoch = \"\"\"    # Only set epoch for DistributedSampler\n",
        "    if hasattr(train_loader.sampler, 'set_epoch'):\n",
        "        train_loader.sampler.set_epoch(epoch)\"\"\"\n",
        "\n",
        "content = content.replace(old_set_epoch, new_set_epoch)\n",
        "\n",
        "# Fix .cuda() call\n",
        "old_cuda = \"    lm_net = lm_net.cuda()\"\n",
        "new_cuda = \"    lm_net = lm_net.to(args.device)\"\n",
        "\n",
        "content = content.replace(old_cuda, new_cuda)\n",
        "\n",
        "with open(gpt2_ft_path, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"✓ gpt2_ft.py patched for Colab!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrswYtDdOI2g",
        "outputId": "d154fd4d-3b7b-4dd8-d834-1807f885b96a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ gpt2_ft.py patched for Colab!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Patch distributed_opt\n",
        "gpu_py_path = \"/content/LoRA/examples/NLG/src/gpu.py\"\n",
        "\n",
        "with open(gpu_py_path, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Fix distributed_opt\n",
        "old_distributed_opt = \"\"\"def distributed_opt(args, model, opt, grad_acc=1):\n",
        "    if args.platform == 'azure':\n",
        "        args.hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
        "        opt = args.hvd.DistributedOptimizer(\n",
        "            opt, named_parameters=model.named_parameters(), backward_passes_per_step=grad_acc\n",
        "        )\n",
        "    elif args.platform == 'philly' or args.platform == 'k8s' or args.platform == 'local':\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank,\n",
        "            find_unused_parameters=False, broadcast_buffers=False\n",
        "        )\n",
        "    return model, opt\"\"\"\n",
        "\n",
        "new_distributed_opt = \"\"\"def distributed_opt(args, model, opt, grad_acc=1):\n",
        "    if args.platform == 'azure':\n",
        "        args.hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
        "        opt = args.hvd.DistributedOptimizer(\n",
        "            opt, named_parameters=model.named_parameters(), backward_passes_per_step=grad_acc\n",
        "        )\n",
        "    elif args.platform in ['philly', 'k8s', 'local'] and args.world_size > 1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank,\n",
        "            find_unused_parameters=False, broadcast_buffers=False\n",
        "        )\n",
        "    return model, opt\"\"\"\n",
        "\n",
        "content = content.replace(old_distributed_opt, new_distributed_opt)\n",
        "\n",
        "with open(gpu_py_path, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"✓ distributed_opt patched!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkab7_HMOOsN",
        "outputId": "e9a54a33-aa48-4746-bcf6-6141fb7da63c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ distributed_opt patched!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Patch distributed_sync\n",
        "gpu_py_path = \"/content/LoRA/examples/NLG/src/gpu.py\"\n",
        "\n",
        "with open(gpu_py_path, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Fix distributed_sync\n",
        "old_sync = \"\"\"def distributed_sync(args):\n",
        "    if args.platform == 'azure':\n",
        "        args.hvd.allreduce(torch.tensor(0), name='barrier')\n",
        "    else:\n",
        "        args.dist.barrier()\"\"\"\n",
        "\n",
        "new_sync = \"\"\"def distributed_sync(args):\n",
        "    if args.platform == 'azure':\n",
        "        args.hvd.allreduce(torch.tensor(0), name='barrier')\n",
        "    elif args.dist is not None:\n",
        "        args.dist.barrier()\"\"\"\n",
        "\n",
        "content = content.replace(old_sync, new_sync)\n",
        "\n",
        "with open(gpu_py_path, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"✓ distributed_sync patched!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhl7_vdOOSRI",
        "outputId": "8c07073b-77ea-4675-be58-1dafda7eeba9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ distributed_sync patched!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Giảm batch size\n",
        "!python src/gpt2_ft.py \\\n",
        "    --train_data \"./data/e2e/train.jsonl\" \\\n",
        "    --valid_data \"./data/e2e/valid.jsonl\" \\\n",
        "    --train_batch_size 2 \\\n",
        "    --grad_acc 4 \\\n",
        "    --valid_batch_size 2 \\\n",
        "    --seq_len 256 \\\n",
        "    --model_card gpt2.md \\\n",
        "    --init_checkpoint \"./pretrained_checkpoints/gpt2-medium-pytorch_model.bin\" \\\n",
        "    --platform local \\\n",
        "    --clip 0.0 \\\n",
        "    --lr 0.0002 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --correct_bias \\\n",
        "    --adam_beta2 0.999 \\\n",
        "    --scheduler linear \\\n",
        "    --warmup_step 100 \\\n",
        "    --max_epoch 2 \\\n",
        "    --save_interval 500 \\\n",
        "    --lora_dim 4 \\\n",
        "    --lora_alpha 32 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --label_smooth 0.1 \\\n",
        "    --work_dir \"./trained_models/GPT2_M/e2e\" \\\n",
        "    --random_seed 110\n",
        "\n",
        "print(\"✓ Training with reduced batch size completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAXgEZ64OXqt",
        "outputId": "4d375915-072c-4bd6-b1e7-cd0d3643a0d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myrank: 0 local_rank: 0 device_count: 1 world_size: 1\n",
            "====================================================================================================\n",
            "        - platform : local\n",
            "        - local_rank : 0\n",
            "        - rank : 0\n",
            "        - device : cuda:0\n",
            "        - world_size : 1\n",
            "        - random_seed : 110\n",
            "        - lr : 0.0002\n",
            "        - weight_decay : 0.01\n",
            "        - correct_bias : True\n",
            "        - adam_epislon : 1e-06\n",
            "        - no_decay_bias : False\n",
            "        - adam_beta1 : 0.9\n",
            "        - adam_beta2 : 0.999\n",
            "        - scheduler : linear\n",
            "        - max_step : None\n",
            "        - max_epoch : 2\n",
            "        - warmup_step : 100\n",
            "        - i_steps : 0\n",
            "        - i_lrs : 0.00025\n",
            "        - train_data : ./data/e2e/train.jsonl\n",
            "        - valid_data : ./data/e2e/valid.jsonl\n",
            "        - train_batch_size : 2\n",
            "        - valid_batch_size : 2\n",
            "        - grad_acc : 4\n",
            "        - clip : 0.0\n",
            "        - seq_len : 256\n",
            "        - model_card : gpt2.md\n",
            "        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin\n",
            "        - fp16 : False\n",
            "        - log_interval : 100\n",
            "        - eval_interval : 2000\n",
            "        - save_interval : 500\n",
            "        - work_dir : ./trained_models/GPT2_M/e2e\n",
            "        - lora_dim : 4\n",
            "        - lora_alpha : 32\n",
            "        - obj : clm\n",
            "        - lora_dropout : 0.1\n",
            "        - label_smooth : 0.1\n",
            "        - roll_interval : -1\n",
            "        - roll_lr : 1e-05\n",
            "        - roll_step : 100\n",
            "        - eval_epoch : 1\n",
            "        - dist : None\n",
            "====================================================================================================\n",
            "Experiment dir : ./trained_models/GPT2_M/e2e\n",
            "loading model pretrained weight.\n",
            "set max_step: 42062\n",
            "start to train the model................ 1\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/content/LoRA/examples/NLG/src/optimizer.py:117: UserWarning: This overload of addcdiv_ is deprecated:\n",
            "\taddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcdiv_(Tensor tensor1, Tensor tensor2, *, Number value = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1805.)\n",
            "  p.data.addcdiv_(-step_size, exp_avg, denom)\n",
            "| epoch   1 step      100 |    100 batches | lr 0.0002 | ms/batch 118.77 | loss  5.08 | avg loss  5.71 | ppl 302.25\n",
            "| epoch   1 step      200 |    200 batches | lr 0.0002 | ms/batch 104.39 | loss  3.28 | avg loss  4.03 | ppl 56.32\n",
            "| epoch   1 step      300 |    300 batches | lr 0.000199 | ms/batch 104.10 | loss  3.56 | avg loss  3.29 | ppl 26.93\n",
            "| epoch   1 step      400 |    400 batches | lr 0.000199 | ms/batch 104.15 | loss  2.83 | avg loss  3.13 | ppl 22.95\n",
            "| epoch   1 step      500 |    500 batches | lr 0.000198 | ms/batch 104.13 | loss  3.12 | avg loss  3.03 | ppl 20.68\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.500.pt\n",
            "| epoch   1 step      600 |    600 batches | lr 0.000198 | ms/batch 104.29 | loss  3.08 | avg loss  2.95 | ppl 19.20\n",
            "| epoch   1 step      700 |    700 batches | lr 0.000197 | ms/batch 104.00 | loss  3.42 | avg loss  2.93 | ppl 18.71\n",
            "| epoch   1 step      800 |    800 batches | lr 0.000197 | ms/batch 104.15 | loss  3.29 | avg loss  2.97 | ppl 19.54\n",
            "| epoch   1 step      900 |    900 batches | lr 0.000196 | ms/batch 104.33 | loss  2.97 | avg loss  2.92 | ppl 18.59\n",
            "| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 103.87 | loss  3.68 | avg loss  2.92 | ppl 18.47\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.1000.pt\n",
            "| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 103.87 | loss  2.70 | avg loss  2.89 | ppl 17.95\n",
            "| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 103.82 | loss  2.37 | avg loss  2.87 | ppl 17.64\n",
            "| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 103.63 | loss  2.91 | avg loss  2.86 | ppl 17.52\n",
            "| epoch   1 step     1400 |   1400 batches | lr 0.000194 | ms/batch 103.81 | loss  3.85 | avg loss  2.86 | ppl 17.54\n",
            "| epoch   1 step     1500 |   1500 batches | lr 0.000193 | ms/batch 104.11 | loss  3.11 | avg loss  2.86 | ppl 17.38\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.1500.pt\n",
            "| epoch   1 step     1600 |   1600 batches | lr 0.000193 | ms/batch 104.19 | loss  2.41 | avg loss  2.81 | ppl 16.69\n",
            "| epoch   1 step     1700 |   1700 batches | lr 0.000192 | ms/batch 104.07 | loss  4.32 | avg loss  2.85 | ppl 17.22\n",
            "| epoch   1 step     1800 |   1800 batches | lr 0.000192 | ms/batch 104.17 | loss  2.95 | avg loss  2.84 | ppl 17.05\n",
            "| epoch   1 step     1900 |   1900 batches | lr 0.000191 | ms/batch 104.03 | loss  2.90 | avg loss  2.86 | ppl 17.54\n",
            "| epoch   1 step     2000 |   2000 batches | lr 0.000191 | ms/batch 103.92 | loss  2.54 | avg loss  2.81 | ppl 16.62\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.2000.pt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "eval samples: 0 loss: tensor(1.3246, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(1.1103, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.9598, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0733, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.1855, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1825, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.6341, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.2969, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.6633, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.1479, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0242, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.2534, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.3058, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.9810, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.3495, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.2590, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3921, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6770, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.5176, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.5834, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.3701, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6560, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.6904, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.6772, device='cuda:0')\n",
            "average loss 1.4204921250627056\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   1 at step     2000 | time: 99.07s | valid loss  1.42 | valid ppl  4.14 | best ppl  4.14 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step     2100 |   2100 batches | lr 0.00019 | ms/batch 1094.83 | loss  2.49 | avg loss  2.73 | ppl 15.32\n",
            "| epoch   1 step     2200 |   2200 batches | lr 0.00019 | ms/batch 104.13 | loss  3.23 | avg loss  2.83 | ppl 16.88\n",
            "| epoch   1 step     2300 |   2300 batches | lr 0.00019 | ms/batch 104.07 | loss  2.67 | avg loss  2.85 | ppl 17.21\n",
            "| epoch   1 step     2400 |   2400 batches | lr 0.000189 | ms/batch 103.95 | loss  3.52 | avg loss  2.81 | ppl 16.61\n",
            "| epoch   1 step     2500 |   2500 batches | lr 0.000189 | ms/batch 103.97 | loss  2.56 | avg loss  2.74 | ppl 15.50\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.2500.pt\n",
            "| epoch   1 step     2600 |   2600 batches | lr 0.000188 | ms/batch 104.43 | loss  2.65 | avg loss  2.72 | ppl 15.10\n",
            "| epoch   1 step     2700 |   2700 batches | lr 0.000188 | ms/batch 104.29 | loss  2.28 | avg loss  2.76 | ppl 15.83\n",
            "| epoch   1 step     2800 |   2800 batches | lr 0.000187 | ms/batch 104.13 | loss  2.81 | avg loss  2.75 | ppl 15.60\n",
            "| epoch   1 step     2900 |   2900 batches | lr 0.000187 | ms/batch 104.00 | loss  2.44 | avg loss  2.81 | ppl 16.56\n",
            "| epoch   1 step     3000 |   3000 batches | lr 0.000186 | ms/batch 104.06 | loss  2.35 | avg loss  2.78 | ppl 16.05\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.3000.pt\n",
            "| epoch   1 step     3100 |   3100 batches | lr 0.000186 | ms/batch 104.34 | loss  2.78 | avg loss  2.74 | ppl 15.46\n",
            "| epoch   1 step     3200 |   3200 batches | lr 0.000185 | ms/batch 104.21 | loss  2.69 | avg loss  2.72 | ppl 15.19\n",
            "| epoch   1 step     3300 |   3300 batches | lr 0.000185 | ms/batch 103.97 | loss  3.27 | avg loss  2.75 | ppl 15.70\n",
            "| epoch   1 step     3400 |   3400 batches | lr 0.000184 | ms/batch 104.08 | loss  2.53 | avg loss  2.80 | ppl 16.42\n",
            "| epoch   1 step     3500 |   3500 batches | lr 0.000184 | ms/batch 103.94 | loss  2.50 | avg loss  2.69 | ppl 14.78\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.3500.pt\n",
            "| epoch   1 step     3600 |   3600 batches | lr 0.000183 | ms/batch 104.03 | loss  3.15 | avg loss  2.77 | ppl 16.02\n",
            "| epoch   1 step     3700 |   3700 batches | lr 0.000183 | ms/batch 104.33 | loss  2.23 | avg loss  2.80 | ppl 16.39\n",
            "| epoch   1 step     3800 |   3800 batches | lr 0.000182 | ms/batch 104.55 | loss  2.43 | avg loss  2.73 | ppl 15.36\n",
            "| epoch   1 step     3900 |   3900 batches | lr 0.000182 | ms/batch 104.04 | loss  2.61 | avg loss  2.78 | ppl 16.09\n",
            "| epoch   1 step     4000 |   4000 batches | lr 0.000181 | ms/batch 104.22 | loss  2.69 | avg loss  2.70 | ppl 14.94\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.4000.pt\n",
            "eval samples: 0 loss: tensor(1.2450, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(1.0392, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8966, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0691, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.0866, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1506, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4504, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.1913, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.6089, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0918, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0983, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.1779, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.2044, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8861, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.2854, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.2032, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3942, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6586, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.4989, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.5338, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2904, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5751, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.5491, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.5425, device='cuda:0')\n",
            "average loss 1.3697968137335697\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   2 at step     4000 | time: 98.75s | valid loss  1.37 | valid ppl  3.93 | best ppl  3.93 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step     4100 |   4100 batches | lr 0.000181 | ms/batch 1091.44 | loss  2.91 | avg loss  2.76 | ppl 15.87\n",
            "| epoch   1 step     4200 |   4200 batches | lr 0.00018 | ms/batch 104.11 | loss  2.37 | avg loss  2.68 | ppl 14.61\n",
            "| epoch   1 step     4300 |   4300 batches | lr 0.00018 | ms/batch 103.92 | loss  2.95 | avg loss  2.75 | ppl 15.60\n",
            "| epoch   1 step     4400 |   4400 batches | lr 0.00018 | ms/batch 104.21 | loss  2.47 | avg loss  2.71 | ppl 15.01\n",
            "| epoch   1 step     4500 |   4500 batches | lr 0.000179 | ms/batch 104.19 | loss  2.25 | avg loss  2.74 | ppl 15.50\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.4500.pt\n",
            "| epoch   1 step     4600 |   4600 batches | lr 0.000179 | ms/batch 104.06 | loss  2.68 | avg loss  2.63 | ppl 13.84\n",
            "| epoch   1 step     4700 |   4700 batches | lr 0.000178 | ms/batch 104.05 | loss  2.73 | avg loss  2.76 | ppl 15.83\n",
            "| epoch   1 step     4800 |   4800 batches | lr 0.000178 | ms/batch 104.18 | loss  2.53 | avg loss  2.70 | ppl 14.85\n",
            "| epoch   1 step     4900 |   4900 batches | lr 0.000177 | ms/batch 104.40 | loss  2.46 | avg loss  2.71 | ppl 15.03\n",
            "| epoch   1 step     5000 |   5000 batches | lr 0.000177 | ms/batch 104.40 | loss  2.76 | avg loss  2.81 | ppl 16.57\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.5000.pt\n",
            "| epoch   1 step     5100 |   5100 batches | lr 0.000176 | ms/batch 104.63 | loss  2.54 | avg loss  2.69 | ppl 14.66\n",
            "| epoch   1 step     5200 |   5200 batches | lr 0.000176 | ms/batch 104.18 | loss  3.57 | avg loss  2.67 | ppl 14.39\n",
            "| epoch   1 step     5300 |   5300 batches | lr 0.000175 | ms/batch 104.10 | loss  2.49 | avg loss  2.77 | ppl 15.95\n",
            "| epoch   1 step     5400 |   5400 batches | lr 0.000175 | ms/batch 103.97 | loss  2.78 | avg loss  2.66 | ppl 14.28\n",
            "| epoch   1 step     5500 |   5500 batches | lr 0.000174 | ms/batch 103.78 | loss  2.46 | avg loss  2.66 | ppl 14.34\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.5500.pt\n",
            "| epoch   1 step     5600 |   5600 batches | lr 0.000174 | ms/batch 104.28 | loss  2.56 | avg loss  2.74 | ppl 15.45\n",
            "| epoch   1 step     5700 |   5700 batches | lr 0.000173 | ms/batch 103.85 | loss  2.54 | avg loss  2.71 | ppl 14.96\n",
            "| epoch   1 step     5800 |   5800 batches | lr 0.000173 | ms/batch 103.97 | loss  2.32 | avg loss  2.66 | ppl 14.37\n",
            "| epoch   1 step     5900 |   5900 batches | lr 0.000172 | ms/batch 103.91 | loss  2.39 | avg loss  2.73 | ppl 15.26\n",
            "| epoch   1 step     6000 |   6000 batches | lr 0.000172 | ms/batch 103.75 | loss  2.50 | avg loss  2.70 | ppl 14.93\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.6000.pt\n",
            "eval samples: 0 loss: tensor(1.2481, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(1.0005, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.9213, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(0.9870, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.0841, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1250, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4972, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.1279, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5822, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0728, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0836, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0780, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.1219, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8654, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.2519, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.1268, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.4621, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6381, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.3734, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.5361, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2613, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5750, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.5181, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.5592, device='cuda:0')\n",
            "average loss 1.3374718768930394\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   3 at step     6000 | time: 98.77s | valid loss  1.34 | valid ppl  3.81 | best ppl  3.81 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step     6100 |   6100 batches | lr 0.000171 | ms/batch 1092.09 | loss  3.06 | avg loss  2.75 | ppl 15.69\n",
            "| epoch   1 step     6200 |   6200 batches | lr 0.000171 | ms/batch 104.24 | loss  2.54 | avg loss  2.67 | ppl 14.38\n",
            "| epoch   1 step     6300 |   6300 batches | lr 0.00017 | ms/batch 104.07 | loss  2.16 | avg loss  2.70 | ppl 14.90\n",
            "| epoch   1 step     6400 |   6400 batches | lr 0.00017 | ms/batch 104.11 | loss  2.67 | avg loss  2.67 | ppl 14.42\n",
            "| epoch   1 step     6500 |   6500 batches | lr 0.000169 | ms/batch 104.10 | loss  2.49 | avg loss  2.64 | ppl 13.98\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.6500.pt\n",
            "| epoch   1 step     6600 |   6600 batches | lr 0.000169 | ms/batch 104.39 | loss  3.43 | avg loss  2.62 | ppl 13.79\n",
            "| epoch   1 step     6700 |   6700 batches | lr 0.000169 | ms/batch 104.12 | loss  2.42 | avg loss  2.64 | ppl 14.05\n",
            "| epoch   1 step     6800 |   6800 batches | lr 0.000168 | ms/batch 103.97 | loss  2.11 | avg loss  2.67 | ppl 14.41\n",
            "| epoch   1 step     6900 |   6900 batches | lr 0.000168 | ms/batch 103.83 | loss  2.85 | avg loss  2.69 | ppl 14.76\n",
            "| epoch   1 step     7000 |   7000 batches | lr 0.000167 | ms/batch 104.16 | loss  2.62 | avg loss  2.63 | ppl 13.89\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.7000.pt\n",
            "| epoch   1 step     7100 |   7100 batches | lr 0.000167 | ms/batch 104.02 | loss  2.24 | avg loss  2.71 | ppl 14.97\n",
            "| epoch   1 step     7200 |   7200 batches | lr 0.000166 | ms/batch 104.23 | loss  2.68 | avg loss  2.72 | ppl 15.12\n",
            "| epoch   1 step     7300 |   7300 batches | lr 0.000166 | ms/batch 104.08 | loss  2.50 | avg loss  2.64 | ppl 14.04\n",
            "| epoch   1 step     7400 |   7400 batches | lr 0.000165 | ms/batch 103.77 | loss  3.04 | avg loss  2.67 | ppl 14.50\n",
            "| epoch   1 step     7500 |   7500 batches | lr 0.000165 | ms/batch 103.72 | loss  2.53 | avg loss  2.68 | ppl 14.64\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.7500.pt\n",
            "| epoch   1 step     7600 |   7600 batches | lr 0.000164 | ms/batch 103.95 | loss  2.84 | avg loss  2.66 | ppl 14.35\n",
            "| epoch   1 step     7700 |   7700 batches | lr 0.000164 | ms/batch 103.76 | loss  2.92 | avg loss  2.66 | ppl 14.25\n",
            "| epoch   1 step     7800 |   7800 batches | lr 0.000163 | ms/batch 103.85 | loss  2.43 | avg loss  2.75 | ppl 15.64\n",
            "| epoch   1 step     7900 |   7900 batches | lr 0.000163 | ms/batch 104.30 | loss  2.55 | avg loss  2.68 | ppl 14.54\n",
            "| epoch   1 step     8000 |   8000 batches | lr 0.000162 | ms/batch 104.34 | loss  2.79 | avg loss  2.65 | ppl 14.14\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.8000.pt\n",
            "eval samples: 0 loss: tensor(1.2304, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.9474, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8827, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0671, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.0109, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1165, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4630, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0866, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5541, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.1055, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0056, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0968, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.1254, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8908, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.2446, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.2211, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3325, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6271, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.3836, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.4514, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2775, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5579, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.4589, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.5209, device='cuda:0')\n",
            "average loss 1.3053808995739442\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   4 at step     8000 | time: 98.78s | valid loss  1.31 | valid ppl  3.69 | best ppl  3.69 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step     8100 |   8100 batches | lr 0.000162 | ms/batch 1091.76 | loss  2.66 | avg loss  2.66 | ppl 14.35\n",
            "| epoch   1 step     8200 |   8200 batches | lr 0.000161 | ms/batch 104.49 | loss  2.11 | avg loss  2.68 | ppl 14.59\n",
            "| epoch   1 step     8300 |   8300 batches | lr 0.000161 | ms/batch 103.94 | loss  2.92 | avg loss  2.69 | ppl 14.68\n",
            "| epoch   1 step     8400 |   8400 batches | lr 0.00016 | ms/batch 104.21 | loss  2.18 | avg loss  2.75 | ppl 15.59\n",
            "| epoch   1 step     8500 |   8500 batches | lr 0.00016 | ms/batch 104.13 | loss  3.16 | avg loss  2.72 | ppl 15.21\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.8500.pt\n",
            "| epoch   1 step     8600 |   8600 batches | lr 0.000159 | ms/batch 104.26 | loss  2.54 | avg loss  2.64 | ppl 14.01\n",
            "| epoch   1 step     8700 |   8700 batches | lr 0.000159 | ms/batch 104.47 | loss  2.60 | avg loss  2.70 | ppl 14.92\n",
            "| epoch   1 step     8800 |   8800 batches | lr 0.000159 | ms/batch 104.30 | loss  3.09 | avg loss  2.70 | ppl 14.82\n",
            "| epoch   1 step     8900 |   8900 batches | lr 0.000158 | ms/batch 104.24 | loss  3.34 | avg loss  2.68 | ppl 14.57\n",
            "| epoch   1 step     9000 |   9000 batches | lr 0.000158 | ms/batch 104.05 | loss  2.34 | avg loss  2.67 | ppl 14.37\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.9000.pt\n",
            "| epoch   1 step     9100 |   9100 batches | lr 0.000157 | ms/batch 104.21 | loss  2.16 | avg loss  2.62 | ppl 13.73\n",
            "| epoch   1 step     9200 |   9200 batches | lr 0.000157 | ms/batch 104.06 | loss  2.27 | avg loss  2.61 | ppl 13.65\n",
            "| epoch   1 step     9300 |   9300 batches | lr 0.000156 | ms/batch 104.30 | loss  3.04 | avg loss  2.63 | ppl 13.89\n",
            "| epoch   1 step     9400 |   9400 batches | lr 0.000156 | ms/batch 104.13 | loss  2.59 | avg loss  2.62 | ppl 13.80\n",
            "| epoch   1 step     9500 |   9500 batches | lr 0.000155 | ms/batch 104.30 | loss  3.06 | avg loss  2.74 | ppl 15.46\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.9500.pt\n",
            "| epoch   1 step     9600 |   9600 batches | lr 0.000155 | ms/batch 104.29 | loss  2.82 | avg loss  2.59 | ppl 13.39\n",
            "| epoch   1 step     9700 |   9700 batches | lr 0.000154 | ms/batch 104.16 | loss  2.68 | avg loss  2.64 | ppl 14.03\n",
            "| epoch   1 step     9800 |   9800 batches | lr 0.000154 | ms/batch 104.64 | loss  2.45 | avg loss  2.68 | ppl 14.57\n",
            "| epoch   1 step     9900 |   9900 batches | lr 0.000153 | ms/batch 104.21 | loss  3.18 | avg loss  2.73 | ppl 15.34\n",
            "| epoch   1 step    10000 |  10000 batches | lr 0.000153 | ms/batch 103.69 | loss  1.96 | avg loss  2.66 | ppl 14.25\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.10000.pt\n",
            "eval samples: 0 loss: tensor(1.1869, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8150, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8835, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0710, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9948, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1164, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4618, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.1108, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5216, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0932, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0893, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.1037, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0849, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8817, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.2191, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.1484, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3709, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6091, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.3625, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.4531, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2549, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5697, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.4189, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4881, device='cuda:0')\n",
            "average loss 1.3030071841323212\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   5 at step    10000 | time: 98.76s | valid loss  1.30 | valid ppl  3.68 | best ppl  3.68 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step    10100 |  10100 batches | lr 0.000152 | ms/batch 1091.57 | loss  2.22 | avg loss  2.67 | ppl 14.51\n",
            "| epoch   1 step    10200 |  10200 batches | lr 0.000152 | ms/batch 104.15 | loss  2.89 | avg loss  2.68 | ppl 14.66\n",
            "| epoch   1 step    10300 |  10300 batches | lr 0.000151 | ms/batch 104.11 | loss  2.68 | avg loss  2.67 | ppl 14.44\n",
            "| epoch   1 step    10400 |  10400 batches | lr 0.000151 | ms/batch 104.27 | loss  2.25 | avg loss  2.56 | ppl 12.98\n",
            "| epoch   1 step    10500 |  10500 batches | lr 0.00015 | ms/batch 104.28 | loss  2.34 | avg loss  2.56 | ppl 12.95\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.10500.pt\n",
            "| epoch   1 step    10600 |  10600 batches | lr 0.00015 | ms/batch 104.37 | loss  2.60 | avg loss  2.69 | ppl 14.72\n",
            "| epoch   1 step    10700 |  10700 batches | lr 0.000149 | ms/batch 104.25 | loss  3.44 | avg loss  2.61 | ppl 13.60\n",
            "| epoch   1 step    10800 |  10800 batches | lr 0.000149 | ms/batch 104.01 | loss  2.31 | avg loss  2.58 | ppl 13.18\n",
            "| epoch   1 step    10900 |  10900 batches | lr 0.000149 | ms/batch 104.32 | loss  2.42 | avg loss  2.66 | ppl 14.32\n",
            "| epoch   1 step    11000 |  11000 batches | lr 0.000148 | ms/batch 103.80 | loss  2.07 | avg loss  2.71 | ppl 15.01\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.11000.pt\n",
            "| epoch   1 step    11100 |  11100 batches | lr 0.000148 | ms/batch 104.29 | loss  2.42 | avg loss  2.59 | ppl 13.40\n",
            "| epoch   1 step    11200 |  11200 batches | lr 0.000147 | ms/batch 104.01 | loss  2.93 | avg loss  2.63 | ppl 13.92\n",
            "| epoch   1 step    11300 |  11300 batches | lr 0.000147 | ms/batch 104.19 | loss  2.07 | avg loss  2.67 | ppl 14.37\n",
            "| epoch   1 step    11400 |  11400 batches | lr 0.000146 | ms/batch 104.45 | loss  2.88 | avg loss  2.62 | ppl 13.75\n",
            "| epoch   1 step    11500 |  11500 batches | lr 0.000146 | ms/batch 104.09 | loss  2.49 | avg loss  2.64 | ppl 14.08\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.11500.pt\n",
            "| epoch   1 step    11600 |  11600 batches | lr 0.000145 | ms/batch 104.39 | loss  2.48 | avg loss  2.61 | ppl 13.63\n",
            "| epoch   1 step    11700 |  11700 batches | lr 0.000145 | ms/batch 103.69 | loss  2.72 | avg loss  2.57 | ppl 13.09\n",
            "| epoch   1 step    11800 |  11800 batches | lr 0.000144 | ms/batch 103.91 | loss  2.59 | avg loss  2.68 | ppl 14.54\n",
            "| epoch   1 step    11900 |  11900 batches | lr 0.000144 | ms/batch 103.83 | loss  2.59 | avg loss  2.75 | ppl 15.62\n",
            "| epoch   1 step    12000 |  12000 batches | lr 0.000143 | ms/batch 104.20 | loss  2.24 | avg loss  2.63 | ppl 13.85\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.12000.pt\n",
            "eval samples: 0 loss: tensor(1.1698, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8102, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8460, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0294, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.0491, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1236, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4691, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0591, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5352, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0437, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0756, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0940, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0772, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8710, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1942, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.1016, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3364, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6322, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.3566, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.4962, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2489, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5982, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.4222, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4383, device='cuda:0')\n",
            "average loss 1.2935866465001074\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   6 at step    12000 | time: 98.76s | valid loss  1.29 | valid ppl  3.65 | best ppl  3.65 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step    12100 |  12100 batches | lr 0.000143 | ms/batch 1091.70 | loss  2.69 | avg loss  2.59 | ppl 13.35\n",
            "| epoch   1 step    12200 |  12200 batches | lr 0.000142 | ms/batch 103.76 | loss  2.36 | avg loss  2.63 | ppl 13.91\n",
            "| epoch   1 step    12300 |  12300 batches | lr 0.000142 | ms/batch 103.88 | loss  4.08 | avg loss  2.61 | ppl 13.65\n",
            "| epoch   1 step    12400 |  12400 batches | lr 0.000141 | ms/batch 103.87 | loss  2.51 | avg loss  2.60 | ppl 13.47\n",
            "| epoch   1 step    12500 |  12500 batches | lr 0.000141 | ms/batch 104.02 | loss  2.49 | avg loss  2.68 | ppl 14.56\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.12500.pt\n",
            "| epoch   1 step    12600 |  12600 batches | lr 0.00014 | ms/batch 104.31 | loss  2.26 | avg loss  2.57 | ppl 13.02\n",
            "| epoch   1 step    12700 |  12700 batches | lr 0.00014 | ms/batch 104.13 | loss  2.44 | avg loss  2.63 | ppl 13.88\n",
            "| epoch   1 step    12800 |  12800 batches | lr 0.000139 | ms/batch 104.16 | loss  2.66 | avg loss  2.62 | ppl 13.79\n",
            "| epoch   1 step    12900 |  12900 batches | lr 0.000139 | ms/batch 104.04 | loss  2.58 | avg loss  2.61 | ppl 13.64\n",
            "| epoch   1 step    13000 |  13000 batches | lr 0.000139 | ms/batch 104.07 | loss  2.48 | avg loss  2.63 | ppl 13.84\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.13000.pt\n",
            "| epoch   1 step    13100 |  13100 batches | lr 0.000138 | ms/batch 104.16 | loss  2.14 | avg loss  2.57 | ppl 13.03\n",
            "| epoch   1 step    13200 |  13200 batches | lr 0.000138 | ms/batch 103.89 | loss  2.20 | avg loss  2.59 | ppl 13.28\n",
            "| epoch   1 step    13300 |  13300 batches | lr 0.000137 | ms/batch 103.95 | loss  2.88 | avg loss  2.63 | ppl 13.88\n",
            "| epoch   1 step    13400 |  13400 batches | lr 0.000137 | ms/batch 104.45 | loss  2.17 | avg loss  2.64 | ppl 14.03\n",
            "| epoch   1 step    13500 |  13500 batches | lr 0.000136 | ms/batch 104.19 | loss  2.83 | avg loss  2.68 | ppl 14.55\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.13500.pt\n",
            "| epoch   1 step    13600 |  13600 batches | lr 0.000136 | ms/batch 104.33 | loss  2.75 | avg loss  2.64 | ppl 14.02\n",
            "| epoch   1 step    13700 |  13700 batches | lr 0.000135 | ms/batch 104.14 | loss  2.64 | avg loss  2.62 | ppl 13.70\n",
            "| epoch   1 step    13800 |  13800 batches | lr 0.000135 | ms/batch 104.02 | loss  2.53 | avg loss  2.62 | ppl 13.77\n",
            "| epoch   1 step    13900 |  13900 batches | lr 0.000134 | ms/batch 104.12 | loss  2.25 | avg loss  2.65 | ppl 14.16\n",
            "| epoch   1 step    14000 |  14000 batches | lr 0.000134 | ms/batch 104.14 | loss  3.01 | avg loss  2.63 | ppl 13.92\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.14000.pt\n",
            "eval samples: 0 loss: tensor(1.0966, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8146, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8388, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0721, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9973, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1229, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4319, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0848, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5022, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0798, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0237, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0977, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0756, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8386, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.2410, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.1527, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3126, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6313, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.4340, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3989, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2039, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6405, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.4029, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.5273, device='cuda:0')\n",
            "average loss 1.2881736080481174\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   7 at step    14000 | time: 98.73s | valid loss  1.29 | valid ppl  3.63 | best ppl  3.63 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step    14100 |  14100 batches | lr 0.000133 | ms/batch 1091.44 | loss  2.01 | avg loss  2.62 | ppl 13.71\n",
            "| epoch   1 step    14200 |  14200 batches | lr 0.000133 | ms/batch 104.01 | loss  2.11 | avg loss  2.61 | ppl 13.66\n",
            "| epoch   1 step    14300 |  14300 batches | lr 0.000132 | ms/batch 103.98 | loss  2.55 | avg loss  2.66 | ppl 14.30\n",
            "| epoch   1 step    14400 |  14400 batches | lr 0.000132 | ms/batch 103.88 | loss  2.60 | avg loss  2.71 | ppl 15.10\n",
            "| epoch   1 step    14500 |  14500 batches | lr 0.000131 | ms/batch 104.32 | loss  2.59 | avg loss  2.67 | ppl 14.40\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.14500.pt\n",
            "| epoch   1 step    14600 |  14600 batches | lr 0.000131 | ms/batch 104.29 | loss  2.72 | avg loss  2.65 | ppl 14.20\n",
            "| epoch   1 step    14700 |  14700 batches | lr 0.00013 | ms/batch 104.77 | loss  2.16 | avg loss  2.60 | ppl 13.44\n",
            "| epoch   1 step    14800 |  14800 batches | lr 0.00013 | ms/batch 104.24 | loss  2.40 | avg loss  2.52 | ppl 12.44\n",
            "| epoch   1 step    14900 |  14900 batches | lr 0.000129 | ms/batch 104.00 | loss  2.92 | avg loss  2.65 | ppl 14.15\n",
            "| epoch   1 step    15000 |  15000 batches | lr 0.000129 | ms/batch 104.17 | loss  2.77 | avg loss  2.62 | ppl 13.73\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.15000.pt\n",
            "| epoch   1 step    15100 |  15100 batches | lr 0.000129 | ms/batch 104.09 | loss  2.36 | avg loss  2.62 | ppl 13.69\n",
            "| epoch   1 step    15200 |  15200 batches | lr 0.000128 | ms/batch 104.24 | loss  2.21 | avg loss  2.67 | ppl 14.51\n",
            "| epoch   1 step    15300 |  15300 batches | lr 0.000128 | ms/batch 103.89 | loss  2.50 | avg loss  2.67 | ppl 14.48\n",
            "| epoch   1 step    15400 |  15400 batches | lr 0.000127 | ms/batch 104.19 | loss  2.64 | avg loss  2.67 | ppl 14.47\n",
            "| epoch   1 step    15500 |  15500 batches | lr 0.000127 | ms/batch 104.23 | loss  2.78 | avg loss  2.68 | ppl 14.61\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.15500.pt\n",
            "| epoch   1 step    15600 |  15600 batches | lr 0.000126 | ms/batch 104.26 | loss  2.56 | avg loss  2.60 | ppl 13.41\n",
            "| epoch   1 step    15700 |  15700 batches | lr 0.000126 | ms/batch 104.35 | loss  2.46 | avg loss  2.58 | ppl 13.14\n",
            "| epoch   1 step    15800 |  15800 batches | lr 0.000125 | ms/batch 104.24 | loss  2.97 | avg loss  2.65 | ppl 14.22\n",
            "| epoch   1 step    15900 |  15900 batches | lr 0.000125 | ms/batch 104.37 | loss  3.26 | avg loss  2.61 | ppl 13.66\n",
            "| epoch   1 step    16000 |  16000 batches | lr 0.000124 | ms/batch 104.12 | loss  2.56 | avg loss  2.51 | ppl 12.33\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.16000.pt\n",
            "eval samples: 0 loss: tensor(1.1533, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.7485, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8362, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0515, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9939, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0907, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4517, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0549, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4772, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0133, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0213, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0777, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0248, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8651, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1521, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0575, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3081, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6070, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2801, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3996, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2349, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5815, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3918, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4322, device='cuda:0')\n",
            "average loss 1.2604700111822313\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   8 at step    16000 | time: 98.72s | valid loss  1.26 | valid ppl  3.53 | best ppl  3.53 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step    16100 |  16100 batches | lr 0.000124 | ms/batch 1091.62 | loss  2.47 | avg loss  2.61 | ppl 13.55\n",
            "| epoch   1 step    16200 |  16200 batches | lr 0.000123 | ms/batch 104.64 | loss  3.21 | avg loss  2.64 | ppl 13.95\n",
            "| epoch   1 step    16300 |  16300 batches | lr 0.000123 | ms/batch 103.96 | loss  2.21 | avg loss  2.56 | ppl 12.89\n",
            "| epoch   1 step    16400 |  16400 batches | lr 0.000122 | ms/batch 104.10 | loss  2.43 | avg loss  2.55 | ppl 12.84\n",
            "| epoch   1 step    16500 |  16500 batches | lr 0.000122 | ms/batch 103.97 | loss  2.76 | avg loss  2.62 | ppl 13.79\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.16500.pt\n",
            "| epoch   1 step    16600 |  16600 batches | lr 0.000121 | ms/batch 104.32 | loss  2.46 | avg loss  2.66 | ppl 14.32\n",
            "| epoch   1 step    16700 |  16700 batches | lr 0.000121 | ms/batch 104.56 | loss  2.79 | avg loss  2.60 | ppl 13.53\n",
            "| epoch   1 step    16800 |  16800 batches | lr 0.00012 | ms/batch 104.50 | loss  2.76 | avg loss  2.59 | ppl 13.38\n",
            "| epoch   1 step    16900 |  16900 batches | lr 0.00012 | ms/batch 104.21 | loss  3.80 | avg loss  2.62 | ppl 13.79\n",
            "| epoch   1 step    17000 |  17000 batches | lr 0.000119 | ms/batch 104.53 | loss  2.47 | avg loss  2.69 | ppl 14.72\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.17000.pt\n",
            "| epoch   1 step    17100 |  17100 batches | lr 0.000119 | ms/batch 104.51 | loss  2.63 | avg loss  2.66 | ppl 14.23\n",
            "| epoch   1 step    17200 |  17200 batches | lr 0.000118 | ms/batch 104.35 | loss  3.10 | avg loss  2.62 | ppl 13.79\n",
            "| epoch   1 step    17300 |  17300 batches | lr 0.000118 | ms/batch 104.44 | loss  2.23 | avg loss  2.60 | ppl 13.47\n",
            "| epoch   1 step    17400 |  17400 batches | lr 0.000118 | ms/batch 103.84 | loss  2.12 | avg loss  2.56 | ppl 12.91\n",
            "| epoch   1 step    17500 |  17500 batches | lr 0.000117 | ms/batch 104.06 | loss  2.60 | avg loss  2.57 | ppl 13.03\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.17500.pt\n",
            "| epoch   1 step    17600 |  17600 batches | lr 0.000117 | ms/batch 104.42 | loss  3.27 | avg loss  2.65 | ppl 14.18\n",
            "| epoch   1 step    17700 |  17700 batches | lr 0.000116 | ms/batch 104.40 | loss  2.48 | avg loss  2.66 | ppl 14.31\n",
            "| epoch   1 step    17800 |  17800 batches | lr 0.000116 | ms/batch 104.46 | loss  2.10 | avg loss  2.63 | ppl 13.82\n",
            "| epoch   1 step    17900 |  17900 batches | lr 0.000115 | ms/batch 104.33 | loss  2.65 | avg loss  2.62 | ppl 13.77\n",
            "| epoch   1 step    18000 |  18000 batches | lr 0.000115 | ms/batch 103.87 | loss  2.10 | avg loss  2.56 | ppl 12.95\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.18000.pt\n",
            "eval samples: 0 loss: tensor(1.1241, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.6648, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8627, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1264, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.0011, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1033, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4176, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0817, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5120, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0601, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0282, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0857, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0278, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8621, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1983, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.1277, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3008, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6129, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2970, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3842, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2111, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5831, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3916, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4113, device='cuda:0')\n",
            "average loss 1.2672614894730792\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   9 at step    18000 | time: 98.66s | valid loss  1.27 | valid ppl  3.55 | best ppl  3.53 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step    18100 |  18100 batches | lr 0.000114 | ms/batch 1090.46 | loss  3.31 | avg loss  2.55 | ppl 12.81\n",
            "| epoch   1 step    18200 |  18200 batches | lr 0.000114 | ms/batch 103.93 | loss  2.12 | avg loss  2.65 | ppl 14.12\n",
            "| epoch   1 step    18300 |  18300 batches | lr 0.000113 | ms/batch 103.85 | loss  2.21 | avg loss  2.59 | ppl 13.27\n",
            "| epoch   1 step    18400 |  18400 batches | lr 0.000113 | ms/batch 103.86 | loss  2.14 | avg loss  2.61 | ppl 13.65\n",
            "| epoch   1 step    18500 |  18500 batches | lr 0.000112 | ms/batch 104.13 | loss  2.71 | avg loss  2.61 | ppl 13.58\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.18500.pt\n",
            "| epoch   1 step    18600 |  18600 batches | lr 0.000112 | ms/batch 104.34 | loss  2.10 | avg loss  2.55 | ppl 12.82\n",
            "| epoch   1 step    18700 |  18700 batches | lr 0.000111 | ms/batch 104.32 | loss  2.30 | avg loss  2.62 | ppl 13.72\n",
            "| epoch   1 step    18800 |  18800 batches | lr 0.000111 | ms/batch 104.11 | loss  2.38 | avg loss  2.55 | ppl 12.76\n",
            "| epoch   1 step    18900 |  18900 batches | lr 0.00011 | ms/batch 104.35 | loss  2.35 | avg loss  2.59 | ppl 13.37\n",
            "| epoch   1 step    19000 |  19000 batches | lr 0.00011 | ms/batch 104.10 | loss  2.55 | avg loss  2.57 | ppl 13.07\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.19000.pt\n",
            "| epoch   1 step    19100 |  19100 batches | lr 0.000109 | ms/batch 104.18 | loss  2.78 | avg loss  2.57 | ppl 13.11\n",
            "| epoch   1 step    19200 |  19200 batches | lr 0.000109 | ms/batch 104.17 | loss  2.62 | avg loss  2.58 | ppl 13.20\n",
            "| epoch   1 step    19300 |  19300 batches | lr 0.000108 | ms/batch 104.60 | loss  2.36 | avg loss  2.67 | ppl 14.44\n",
            "| epoch   1 step    19400 |  19400 batches | lr 0.000108 | ms/batch 104.18 | loss  2.67 | avg loss  2.64 | ppl 14.03\n",
            "| epoch   1 step    19500 |  19500 batches | lr 0.000108 | ms/batch 104.26 | loss  2.43 | avg loss  2.57 | ppl 13.06\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.19500.pt\n",
            "| epoch   1 step    19600 |  19600 batches | lr 0.000107 | ms/batch 104.57 | loss  2.14 | avg loss  2.55 | ppl 12.85\n",
            "| epoch   1 step    19700 |  19700 batches | lr 0.000107 | ms/batch 104.27 | loss  2.23 | avg loss  2.62 | ppl 13.78\n",
            "| epoch   1 step    19800 |  19800 batches | lr 0.000106 | ms/batch 104.18 | loss  2.40 | avg loss  2.57 | ppl 13.12\n",
            "| epoch   1 step    19900 |  19900 batches | lr 0.000106 | ms/batch 103.91 | loss  2.51 | avg loss  2.60 | ppl 13.48\n",
            "| epoch   1 step    20000 |  20000 batches | lr 0.000105 | ms/batch 104.22 | loss  2.83 | avg loss  2.63 | ppl 13.83\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.20000.pt\n",
            "eval samples: 0 loss: tensor(1.0899, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.6680, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8201, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0686, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9750, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1249, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4162, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0679, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4743, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0163, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9915, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0582, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0094, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8570, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1782, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0710, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3238, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6111, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2964, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.4165, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2174, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6371, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3611, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4536, device='cuda:0')\n",
            "average loss 1.2493288461501673\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  10 at step    20000 | time: 98.67s | valid loss  1.25 | valid ppl  3.49 | best ppl  3.49 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step    20100 |  20100 batches | lr 0.000105 | ms/batch 1091.18 | loss  2.84 | avg loss  2.63 | ppl 13.90\n",
            "| epoch   1 step    20200 |  20200 batches | lr 0.000104 | ms/batch 104.28 | loss  2.57 | avg loss  2.58 | ppl 13.17\n",
            "| epoch   1 step    20300 |  20300 batches | lr 0.000104 | ms/batch 104.52 | loss  2.39 | avg loss  2.55 | ppl 12.80\n",
            "| epoch   1 step    20400 |  20400 batches | lr 0.000103 | ms/batch 104.29 | loss  2.97 | avg loss  2.55 | ppl 12.81\n",
            "| epoch   1 step    20500 |  20500 batches | lr 0.000103 | ms/batch 104.18 | loss  2.30 | avg loss  2.57 | ppl 13.09\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.20500.pt\n",
            "| epoch   1 step    20600 |  20600 batches | lr 0.000102 | ms/batch 104.22 | loss  2.46 | avg loss  2.60 | ppl 13.50\n",
            "| epoch   1 step    20700 |  20700 batches | lr 0.000102 | ms/batch 104.04 | loss  3.22 | avg loss  2.63 | ppl 13.89\n",
            "| epoch   1 step    20800 |  20800 batches | lr 0.000101 | ms/batch 104.21 | loss  2.51 | avg loss  2.60 | ppl 13.45\n",
            "| epoch   1 step    20900 |  20900 batches | lr 0.000101 | ms/batch 104.21 | loss  2.76 | avg loss  2.61 | ppl 13.61\n",
            "| epoch   1 step    21000 |  21000 batches | lr 0.0001 | ms/batch 103.98 | loss  2.15 | avg loss  2.58 | ppl 13.20\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.21000.pt\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.21031.pt\n",
            "start to train the model................ 2\n",
            "| epoch   2 step    21100 |     69 batches | lr 9.99e-05 | ms/batch 71.95 | loss  2.59 | avg loss  2.57 | ppl 13.02\n",
            "| epoch   2 step    21200 |    169 batches | lr 9.94e-05 | ms/batch 103.91 | loss  3.15 | avg loss  2.62 | ppl 13.67\n",
            "| epoch   2 step    21300 |    269 batches | lr 9.9e-05 | ms/batch 103.88 | loss  2.75 | avg loss  2.56 | ppl 12.92\n",
            "| epoch   2 step    21400 |    369 batches | lr 9.85e-05 | ms/batch 103.98 | loss  2.45 | avg loss  2.55 | ppl 12.78\n",
            "| epoch   2 step    21500 |    469 batches | lr 9.8e-05 | ms/batch 103.99 | loss  2.00 | avg loss  2.56 | ppl 12.87\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.21500.pt\n",
            "| epoch   2 step    21600 |    569 batches | lr 9.75e-05 | ms/batch 103.78 | loss  2.38 | avg loss  2.67 | ppl 14.38\n",
            "| epoch   2 step    21700 |    669 batches | lr 9.7e-05 | ms/batch 103.68 | loss  2.66 | avg loss  2.60 | ppl 13.44\n",
            "| epoch   2 step    21800 |    769 batches | lr 9.66e-05 | ms/batch 103.83 | loss  2.33 | avg loss  2.57 | ppl 13.11\n",
            "| epoch   2 step    21900 |    869 batches | lr 9.61e-05 | ms/batch 103.76 | loss  2.43 | avg loss  2.58 | ppl 13.22\n",
            "| epoch   2 step    22000 |    969 batches | lr 9.56e-05 | ms/batch 104.14 | loss  2.19 | avg loss  2.59 | ppl 13.40\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.22000.pt\n",
            "eval samples: 0 loss: tensor(1.1048, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.6996, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8247, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0510, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(1.0048, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1190, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4161, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0427, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5122, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0385, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0275, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0743, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0246, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8830, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1806, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0950, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3019, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6376, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.3481, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.4626, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2659, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5987, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3460, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4100, device='cuda:0')\n",
            "average loss 1.2515094655833832\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  11 at step    22000 | time: 98.69s | valid loss  1.25 | valid ppl  3.50 | best ppl  3.50 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    22100 |   1069 batches | lr 9.51e-05 | ms/batch 1090.81 | loss  2.16 | avg loss  2.60 | ppl 13.45\n",
            "| epoch   2 step    22200 |   1169 batches | lr 9.47e-05 | ms/batch 104.18 | loss  2.46 | avg loss  2.56 | ppl 12.94\n",
            "| epoch   2 step    22300 |   1269 batches | lr 9.42e-05 | ms/batch 104.12 | loss  2.92 | avg loss  2.62 | ppl 13.78\n",
            "| epoch   2 step    22400 |   1369 batches | lr 9.37e-05 | ms/batch 104.04 | loss  2.38 | avg loss  2.59 | ppl 13.31\n",
            "| epoch   2 step    22500 |   1469 batches | lr 9.32e-05 | ms/batch 104.19 | loss  2.29 | avg loss  2.57 | ppl 13.01\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.22500.pt\n",
            "| epoch   2 step    22600 |   1569 batches | lr 9.28e-05 | ms/batch 104.55 | loss  3.77 | avg loss  2.54 | ppl 12.62\n",
            "| epoch   2 step    22700 |   1669 batches | lr 9.23e-05 | ms/batch 104.64 | loss  2.44 | avg loss  2.59 | ppl 13.38\n",
            "| epoch   2 step    22800 |   1769 batches | lr 9.18e-05 | ms/batch 104.60 | loss  2.21 | avg loss  2.65 | ppl 14.18\n",
            "| epoch   2 step    22900 |   1869 batches | lr 9.13e-05 | ms/batch 104.32 | loss  3.19 | avg loss  2.59 | ppl 13.30\n",
            "| epoch   2 step    23000 |   1969 batches | lr 9.09e-05 | ms/batch 104.04 | loss  2.01 | avg loss  2.56 | ppl 12.92\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.23000.pt\n",
            "| epoch   2 step    23100 |   2069 batches | lr 9.04e-05 | ms/batch 104.57 | loss  2.41 | avg loss  2.55 | ppl 12.81\n",
            "| epoch   2 step    23200 |   2169 batches | lr 8.99e-05 | ms/batch 104.41 | loss  2.59 | avg loss  2.62 | ppl 13.69\n",
            "| epoch   2 step    23300 |   2269 batches | lr 8.94e-05 | ms/batch 103.78 | loss  2.06 | avg loss  2.62 | ppl 13.79\n",
            "| epoch   2 step    23400 |   2369 batches | lr 8.89e-05 | ms/batch 104.37 | loss  2.57 | avg loss  2.59 | ppl 13.33\n",
            "| epoch   2 step    23500 |   2469 batches | lr 8.85e-05 | ms/batch 104.23 | loss  2.58 | avg loss  2.62 | ppl 13.73\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.23500.pt\n",
            "| epoch   2 step    23600 |   2569 batches | lr 8.8e-05 | ms/batch 104.47 | loss  2.50 | avg loss  2.59 | ppl 13.36\n",
            "| epoch   2 step    23700 |   2669 batches | lr 8.75e-05 | ms/batch 104.16 | loss  2.39 | avg loss  2.58 | ppl 13.17\n",
            "| epoch   2 step    23800 |   2769 batches | lr 8.7e-05 | ms/batch 104.21 | loss  2.54 | avg loss  2.59 | ppl 13.31\n",
            "| epoch   2 step    23900 |   2869 batches | lr 8.66e-05 | ms/batch 104.57 | loss  2.43 | avg loss  2.61 | ppl 13.54\n",
            "| epoch   2 step    24000 |   2969 batches | lr 8.61e-05 | ms/batch 104.55 | loss  2.68 | avg loss  2.68 | ppl 14.57\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.24000.pt\n",
            "eval samples: 0 loss: tensor(1.1195, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.6399, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8458, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0615, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9782, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1023, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4367, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0255, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5017, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.0031, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0605, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0633, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9913, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8284, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1657, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0963, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3048, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6059, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.3237, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3877, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2261, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5854, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3989, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3849, device='cuda:0')\n",
            "average loss 1.237493256808654\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  12 at step    24000 | time: 98.68s | valid loss  1.24 | valid ppl  3.45 | best ppl  3.45 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    24100 |   3069 batches | lr 8.56e-05 | ms/batch 1091.31 | loss  2.51 | avg loss  2.59 | ppl 13.28\n",
            "| epoch   2 step    24200 |   3169 batches | lr 8.51e-05 | ms/batch 104.30 | loss  2.62 | avg loss  2.54 | ppl 12.65\n",
            "| epoch   2 step    24300 |   3269 batches | lr 8.47e-05 | ms/batch 104.39 | loss  2.38 | avg loss  2.60 | ppl 13.45\n",
            "| epoch   2 step    24400 |   3369 batches | lr 8.42e-05 | ms/batch 104.54 | loss  2.39 | avg loss  2.60 | ppl 13.48\n",
            "| epoch   2 step    24500 |   3469 batches | lr 8.37e-05 | ms/batch 104.66 | loss  2.36 | avg loss  2.57 | ppl 13.06\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.24500.pt\n",
            "| epoch   2 step    24600 |   3569 batches | lr 8.32e-05 | ms/batch 104.35 | loss  2.48 | avg loss  2.59 | ppl 13.29\n",
            "| epoch   2 step    24700 |   3669 batches | lr 8.28e-05 | ms/batch 104.09 | loss  2.23 | avg loss  2.55 | ppl 12.78\n",
            "| epoch   2 step    24800 |   3769 batches | lr 8.23e-05 | ms/batch 104.08 | loss  2.44 | avg loss  2.62 | ppl 13.71\n",
            "| epoch   2 step    24900 |   3869 batches | lr 8.18e-05 | ms/batch 103.67 | loss  2.84 | avg loss  2.57 | ppl 13.02\n",
            "| epoch   2 step    25000 |   3969 batches | lr 8.13e-05 | ms/batch 104.35 | loss  2.81 | avg loss  2.61 | ppl 13.54\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.25000.pt\n",
            "| epoch   2 step    25100 |   4069 batches | lr 8.08e-05 | ms/batch 104.58 | loss  2.49 | avg loss  2.60 | ppl 13.42\n",
            "| epoch   2 step    25200 |   4169 batches | lr 8.04e-05 | ms/batch 104.09 | loss  2.30 | avg loss  2.54 | ppl 12.63\n",
            "| epoch   2 step    25300 |   4269 batches | lr 7.99e-05 | ms/batch 104.19 | loss  2.89 | avg loss  2.58 | ppl 13.19\n",
            "| epoch   2 step    25400 |   4369 batches | lr 7.94e-05 | ms/batch 104.52 | loss  2.43 | avg loss  2.57 | ppl 13.12\n",
            "| epoch   2 step    25500 |   4469 batches | lr 7.89e-05 | ms/batch 104.24 | loss  2.49 | avg loss  2.60 | ppl 13.48\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.25500.pt\n",
            "| epoch   2 step    25600 |   4569 batches | lr 7.85e-05 | ms/batch 104.21 | loss  2.34 | avg loss  2.48 | ppl 11.92\n",
            "| epoch   2 step    25700 |   4669 batches | lr 7.8e-05 | ms/batch 104.18 | loss  2.67 | avg loss  2.58 | ppl 13.19\n",
            "| epoch   2 step    25800 |   4769 batches | lr 7.75e-05 | ms/batch 103.83 | loss  2.08 | avg loss  2.58 | ppl 13.13\n",
            "| epoch   2 step    25900 |   4869 batches | lr 7.7e-05 | ms/batch 104.10 | loss  2.37 | avg loss  2.57 | ppl 13.11\n",
            "| epoch   2 step    26000 |   4969 batches | lr 7.66e-05 | ms/batch 104.06 | loss  2.26 | avg loss  2.59 | ppl 13.31\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.26000.pt\n",
            "eval samples: 0 loss: tensor(1.1150, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.6313, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8234, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0835, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9934, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0950, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4232, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0419, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4763, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9851, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0288, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0801, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0456, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8892, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1633, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0809, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.3176, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6234, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2920, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3797, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2187, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6179, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3717, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3591, device='cuda:0')\n",
            "average loss 1.2414326160788944\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  13 at step    26000 | time: 98.62s | valid loss  1.24 | valid ppl  3.46 | best ppl  3.45 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    26100 |   5069 batches | lr 7.61e-05 | ms/batch 1090.39 | loss  2.47 | avg loss  2.53 | ppl 12.57\n",
            "| epoch   2 step    26200 |   5169 batches | lr 7.56e-05 | ms/batch 104.00 | loss  3.10 | avg loss  2.57 | ppl 13.11\n",
            "| epoch   2 step    26300 |   5269 batches | lr 7.51e-05 | ms/batch 103.78 | loss  2.72 | avg loss  2.56 | ppl 12.90\n",
            "| epoch   2 step    26400 |   5369 batches | lr 7.46e-05 | ms/batch 103.93 | loss  2.78 | avg loss  2.60 | ppl 13.51\n",
            "| epoch   2 step    26500 |   5469 batches | lr 7.42e-05 | ms/batch 104.22 | loss  2.70 | avg loss  2.56 | ppl 12.93\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.26500.pt\n",
            "| epoch   2 step    26600 |   5569 batches | lr 7.37e-05 | ms/batch 104.43 | loss  2.64 | avg loss  2.52 | ppl 12.37\n",
            "| epoch   2 step    26700 |   5669 batches | lr 7.32e-05 | ms/batch 104.48 | loss  2.64 | avg loss  2.65 | ppl 14.10\n",
            "| epoch   2 step    26800 |   5769 batches | lr 7.27e-05 | ms/batch 104.35 | loss  2.80 | avg loss  2.58 | ppl 13.23\n",
            "| epoch   2 step    26900 |   5869 batches | lr 7.23e-05 | ms/batch 104.52 | loss  2.23 | avg loss  2.65 | ppl 14.15\n",
            "| epoch   2 step    27000 |   5969 batches | lr 7.18e-05 | ms/batch 104.52 | loss  2.98 | avg loss  2.58 | ppl 13.20\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.27000.pt\n",
            "| epoch   2 step    27100 |   6069 batches | lr 7.13e-05 | ms/batch 104.42 | loss  2.24 | avg loss  2.65 | ppl 14.14\n",
            "| epoch   2 step    27200 |   6169 batches | lr 7.08e-05 | ms/batch 104.28 | loss  3.63 | avg loss  2.54 | ppl 12.70\n",
            "| epoch   2 step    27300 |   6269 batches | lr 7.04e-05 | ms/batch 104.49 | loss  2.59 | avg loss  2.60 | ppl 13.51\n",
            "| epoch   2 step    27400 |   6369 batches | lr 6.99e-05 | ms/batch 104.44 | loss  2.07 | avg loss  2.55 | ppl 12.85\n",
            "| epoch   2 step    27500 |   6469 batches | lr 6.94e-05 | ms/batch 104.17 | loss  2.80 | avg loss  2.55 | ppl 12.79\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.27500.pt\n",
            "| epoch   2 step    27600 |   6569 batches | lr 6.89e-05 | ms/batch 104.42 | loss  3.06 | avg loss  2.59 | ppl 13.33\n",
            "| epoch   2 step    27700 |   6669 batches | lr 6.85e-05 | ms/batch 104.43 | loss  2.12 | avg loss  2.49 | ppl 12.09\n",
            "| epoch   2 step    27800 |   6769 batches | lr 6.8e-05 | ms/batch 104.53 | loss  2.53 | avg loss  2.62 | ppl 13.68\n",
            "| epoch   2 step    27900 |   6869 batches | lr 6.75e-05 | ms/batch 104.15 | loss  2.38 | avg loss  2.61 | ppl 13.55\n",
            "| epoch   2 step    28000 |   6969 batches | lr 6.7e-05 | ms/batch 104.25 | loss  2.58 | avg loss  2.57 | ppl 13.08\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.28000.pt\n",
            "eval samples: 0 loss: tensor(1.0899, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5896, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8281, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0357, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9841, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1431, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3941, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.9850, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4850, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9837, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(2.0261, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0710, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(1.0031, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8363, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1538, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0746, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2588, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.5969, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2401, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3654, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.1389, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6186, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3710, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.4124, device='cuda:0')\n",
            "average loss 1.2298235641792417\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  14 at step    28000 | time: 98.71s | valid loss  1.23 | valid ppl  3.42 | best ppl  3.42 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    28100 |   7069 batches | lr 6.65e-05 | ms/batch 1091.28 | loss  3.19 | avg loss  2.55 | ppl 12.85\n",
            "| epoch   2 step    28200 |   7169 batches | lr 6.61e-05 | ms/batch 104.15 | loss  2.97 | avg loss  2.60 | ppl 13.44\n",
            "| epoch   2 step    28300 |   7269 batches | lr 6.56e-05 | ms/batch 104.42 | loss  2.50 | avg loss  2.58 | ppl 13.26\n",
            "| epoch   2 step    28400 |   7369 batches | lr 6.51e-05 | ms/batch 104.45 | loss  2.87 | avg loss  2.56 | ppl 12.95\n",
            "| epoch   2 step    28500 |   7469 batches | lr 6.46e-05 | ms/batch 104.35 | loss  2.46 | avg loss  2.54 | ppl 12.67\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.28500.pt\n",
            "| epoch   2 step    28600 |   7569 batches | lr 6.42e-05 | ms/batch 104.35 | loss  2.66 | avg loss  2.56 | ppl 12.96\n",
            "| epoch   2 step    28700 |   7669 batches | lr 6.37e-05 | ms/batch 104.11 | loss  2.15 | avg loss  2.56 | ppl 12.96\n",
            "| epoch   2 step    28800 |   7769 batches | lr 6.32e-05 | ms/batch 104.33 | loss  3.43 | avg loss  2.56 | ppl 12.96\n",
            "| epoch   2 step    28900 |   7869 batches | lr 6.27e-05 | ms/batch 104.22 | loss  2.39 | avg loss  2.54 | ppl 12.66\n",
            "| epoch   2 step    29000 |   7969 batches | lr 6.23e-05 | ms/batch 104.50 | loss  2.50 | avg loss  2.58 | ppl 13.22\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.29000.pt\n",
            "| epoch   2 step    29100 |   8069 batches | lr 6.18e-05 | ms/batch 103.79 | loss  2.25 | avg loss  2.58 | ppl 13.17\n",
            "| epoch   2 step    29200 |   8169 batches | lr 6.13e-05 | ms/batch 104.17 | loss  2.85 | avg loss  2.53 | ppl 12.60\n",
            "| epoch   2 step    29300 |   8269 batches | lr 6.08e-05 | ms/batch 104.06 | loss  2.27 | avg loss  2.57 | ppl 13.09\n",
            "| epoch   2 step    29400 |   8369 batches | lr 6.03e-05 | ms/batch 104.48 | loss  2.42 | avg loss  2.58 | ppl 13.17\n",
            "| epoch   2 step    29500 |   8469 batches | lr 5.99e-05 | ms/batch 104.56 | loss  2.55 | avg loss  2.49 | ppl 12.03\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.29500.pt\n",
            "| epoch   2 step    29600 |   8569 batches | lr 5.94e-05 | ms/batch 104.35 | loss  1.75 | avg loss  2.56 | ppl 12.99\n",
            "| epoch   2 step    29700 |   8669 batches | lr 5.89e-05 | ms/batch 104.21 | loss  2.22 | avg loss  2.52 | ppl 12.43\n",
            "| epoch   2 step    29800 |   8769 batches | lr 5.84e-05 | ms/batch 104.59 | loss  3.06 | avg loss  2.57 | ppl 13.00\n",
            "| epoch   2 step    29900 |   8869 batches | lr 5.8e-05 | ms/batch 104.08 | loss  2.98 | avg loss  2.57 | ppl 13.11\n",
            "| epoch   2 step    30000 |   8969 batches | lr 5.75e-05 | ms/batch 104.30 | loss  2.87 | avg loss  2.55 | ppl 12.86\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.30000.pt\n",
            "eval samples: 0 loss: tensor(1.0560, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5586, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8387, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0705, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9782, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0884, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3883, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0127, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.5156, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9748, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9997, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.1000, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9945, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8694, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1679, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0439, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2729, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.5958, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2588, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3623, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.1787, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6229, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3519, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3378, device='cuda:0')\n",
            "average loss 1.2263795140977591\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  15 at step    30000 | time: 98.67s | valid loss  1.23 | valid ppl  3.41 | best ppl  3.41 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    30100 |   9069 batches | lr 5.7e-05 | ms/batch 1090.97 | loss  2.46 | avg loss  2.60 | ppl 13.49\n",
            "| epoch   2 step    30200 |   9169 batches | lr 5.65e-05 | ms/batch 104.49 | loss  3.37 | avg loss  2.54 | ppl 12.74\n",
            "| epoch   2 step    30300 |   9269 batches | lr 5.61e-05 | ms/batch 104.30 | loss  2.70 | avg loss  2.56 | ppl 12.93\n",
            "| epoch   2 step    30400 |   9369 batches | lr 5.56e-05 | ms/batch 103.96 | loss  2.26 | avg loss  2.57 | ppl 13.10\n",
            "| epoch   2 step    30500 |   9469 batches | lr 5.51e-05 | ms/batch 104.04 | loss  2.64 | avg loss  2.52 | ppl 12.43\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.30500.pt\n",
            "| epoch   2 step    30600 |   9569 batches | lr 5.46e-05 | ms/batch 103.98 | loss  2.91 | avg loss  2.59 | ppl 13.34\n",
            "| epoch   2 step    30700 |   9669 batches | lr 5.42e-05 | ms/batch 103.86 | loss  2.24 | avg loss  2.56 | ppl 12.88\n",
            "| epoch   2 step    30800 |   9769 batches | lr 5.37e-05 | ms/batch 103.79 | loss  3.31 | avg loss  2.63 | ppl 13.83\n",
            "| epoch   2 step    30900 |   9869 batches | lr 5.32e-05 | ms/batch 104.09 | loss  2.57 | avg loss  2.55 | ppl 12.86\n",
            "| epoch   2 step    31000 |   9969 batches | lr 5.27e-05 | ms/batch 104.45 | loss  2.93 | avg loss  2.59 | ppl 13.37\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.31000.pt\n",
            "| epoch   2 step    31100 |  10069 batches | lr 5.22e-05 | ms/batch 103.98 | loss  3.28 | avg loss  2.60 | ppl 13.50\n",
            "| epoch   2 step    31200 |  10169 batches | lr 5.18e-05 | ms/batch 103.89 | loss  2.69 | avg loss  2.58 | ppl 13.14\n",
            "| epoch   2 step    31300 |  10269 batches | lr 5.13e-05 | ms/batch 104.00 | loss  2.22 | avg loss  2.55 | ppl 12.83\n",
            "| epoch   2 step    31400 |  10369 batches | lr 5.08e-05 | ms/batch 104.13 | loss  3.05 | avg loss  2.55 | ppl 12.78\n",
            "| epoch   2 step    31500 |  10469 batches | lr 5.03e-05 | ms/batch 104.21 | loss  2.30 | avg loss  2.54 | ppl 12.69\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.31500.pt\n",
            "| epoch   2 step    31600 |  10569 batches | lr 4.99e-05 | ms/batch 104.08 | loss  2.90 | avg loss  2.55 | ppl 12.79\n",
            "| epoch   2 step    31700 |  10669 batches | lr 4.94e-05 | ms/batch 103.97 | loss  2.33 | avg loss  2.59 | ppl 13.33\n",
            "| epoch   2 step    31800 |  10769 batches | lr 4.89e-05 | ms/batch 103.98 | loss  2.55 | avg loss  2.58 | ppl 13.18\n",
            "| epoch   2 step    31900 |  10869 batches | lr 4.84e-05 | ms/batch 103.90 | loss  2.16 | avg loss  2.52 | ppl 12.44\n",
            "| epoch   2 step    32000 |  10969 batches | lr 4.8e-05 | ms/batch 104.22 | loss  2.54 | avg loss  2.62 | ppl 13.67\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.32000.pt\n",
            "eval samples: 0 loss: tensor(1.1027, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.6168, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8372, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0809, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9841, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0912, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4155, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0173, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4936, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9723, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9885, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0877, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9927, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8717, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1693, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0417, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2853, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.5967, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2850, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3855, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2263, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5763, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3397, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3665, device='cuda:0')\n",
            "average loss 1.2319728574677282\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  16 at step    32000 | time: 98.76s | valid loss  1.23 | valid ppl  3.43 | best ppl  3.41 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    32100 |  11069 batches | lr 4.75e-05 | ms/batch 1091.70 | loss  2.74 | avg loss  2.57 | ppl 13.05\n",
            "| epoch   2 step    32200 |  11169 batches | lr 4.7e-05 | ms/batch 104.25 | loss  3.06 | avg loss  2.53 | ppl 12.52\n",
            "| epoch   2 step    32300 |  11269 batches | lr 4.65e-05 | ms/batch 104.44 | loss  2.52 | avg loss  2.54 | ppl 12.69\n",
            "| epoch   2 step    32400 |  11369 batches | lr 4.61e-05 | ms/batch 104.40 | loss  2.38 | avg loss  2.55 | ppl 12.80\n",
            "| epoch   2 step    32500 |  11469 batches | lr 4.56e-05 | ms/batch 104.62 | loss  2.67 | avg loss  2.55 | ppl 12.79\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.32500.pt\n",
            "| epoch   2 step    32600 |  11569 batches | lr 4.51e-05 | ms/batch 104.45 | loss  2.57 | avg loss  2.50 | ppl 12.22\n",
            "| epoch   2 step    32700 |  11669 batches | lr 4.46e-05 | ms/batch 104.38 | loss  2.96 | avg loss  2.57 | ppl 13.02\n",
            "| epoch   2 step    32800 |  11769 batches | lr 4.41e-05 | ms/batch 104.42 | loss  2.59 | avg loss  2.54 | ppl 12.73\n",
            "| epoch   2 step    32900 |  11869 batches | lr 4.37e-05 | ms/batch 104.31 | loss  2.78 | avg loss  2.63 | ppl 13.84\n",
            "| epoch   2 step    33000 |  11969 batches | lr 4.32e-05 | ms/batch 104.50 | loss  2.12 | avg loss  2.61 | ppl 13.61\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.33000.pt\n",
            "| epoch   2 step    33100 |  12069 batches | lr 4.27e-05 | ms/batch 104.48 | loss  2.30 | avg loss  2.57 | ppl 13.00\n",
            "| epoch   2 step    33200 |  12169 batches | lr 4.22e-05 | ms/batch 104.13 | loss  2.87 | avg loss  2.54 | ppl 12.72\n",
            "| epoch   2 step    33300 |  12269 batches | lr 4.18e-05 | ms/batch 104.07 | loss  3.53 | avg loss  2.53 | ppl 12.56\n",
            "| epoch   2 step    33400 |  12369 batches | lr 4.13e-05 | ms/batch 104.12 | loss  2.60 | avg loss  2.52 | ppl 12.40\n",
            "| epoch   2 step    33500 |  12469 batches | lr 4.08e-05 | ms/batch 104.41 | loss  2.57 | avg loss  2.58 | ppl 13.14\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.33500.pt\n",
            "| epoch   2 step    33600 |  12569 batches | lr 4.03e-05 | ms/batch 104.44 | loss  2.23 | avg loss  2.60 | ppl 13.52\n",
            "| epoch   2 step    33700 |  12669 batches | lr 3.99e-05 | ms/batch 104.38 | loss  2.15 | avg loss  2.58 | ppl 13.21\n",
            "| epoch   2 step    33800 |  12769 batches | lr 3.94e-05 | ms/batch 104.34 | loss  2.60 | avg loss  2.55 | ppl 12.86\n",
            "| epoch   2 step    33900 |  12869 batches | lr 3.89e-05 | ms/batch 104.16 | loss  2.24 | avg loss  2.53 | ppl 12.55\n",
            "| epoch   2 step    34000 |  12969 batches | lr 3.84e-05 | ms/batch 104.50 | loss  2.64 | avg loss  2.57 | ppl 13.02\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.34000.pt\n",
            "eval samples: 0 loss: tensor(1.0715, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5299, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8500, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0602, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9736, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.1083, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.4125, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0059, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4580, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9599, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9793, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0838, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9796, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8641, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1552, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0532, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2546, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6174, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2531, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3732, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.2033, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5826, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3082, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3757, device='cuda:0')\n",
            "average loss 1.2229959150546625\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  17 at step    34000 | time: 98.78s | valid loss  1.22 | valid ppl  3.40 | best ppl  3.40 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    34100 |  13069 batches | lr 3.79e-05 | ms/batch 1092.31 | loss  2.89 | avg loss  2.54 | ppl 12.69\n",
            "| epoch   2 step    34200 |  13169 batches | lr 3.75e-05 | ms/batch 104.79 | loss  2.00 | avg loss  2.54 | ppl 12.72\n",
            "| epoch   2 step    34300 |  13269 batches | lr 3.7e-05 | ms/batch 104.32 | loss  2.47 | avg loss  2.55 | ppl 12.82\n",
            "| epoch   2 step    34400 |  13369 batches | lr 3.65e-05 | ms/batch 104.23 | loss  2.44 | avg loss  2.53 | ppl 12.53\n",
            "| epoch   2 step    34500 |  13469 batches | lr 3.6e-05 | ms/batch 104.07 | loss  2.41 | avg loss  2.64 | ppl 14.06\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.34500.pt\n",
            "| epoch   2 step    34600 |  13569 batches | lr 3.56e-05 | ms/batch 104.10 | loss  2.45 | avg loss  2.58 | ppl 13.21\n",
            "| epoch   2 step    34700 |  13669 batches | lr 3.51e-05 | ms/batch 103.93 | loss  2.58 | avg loss  2.58 | ppl 13.16\n",
            "| epoch   2 step    34800 |  13769 batches | lr 3.46e-05 | ms/batch 103.94 | loss  3.30 | avg loss  2.54 | ppl 12.66\n",
            "| epoch   2 step    34900 |  13869 batches | lr 3.41e-05 | ms/batch 104.00 | loss  2.09 | avg loss  2.55 | ppl 12.75\n",
            "| epoch   2 step    35000 |  13969 batches | lr 3.37e-05 | ms/batch 103.90 | loss  2.74 | avg loss  2.55 | ppl 12.87\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.35000.pt\n",
            "| epoch   2 step    35100 |  14069 batches | lr 3.32e-05 | ms/batch 104.17 | loss  2.80 | avg loss  2.56 | ppl 12.88\n",
            "| epoch   2 step    35200 |  14169 batches | lr 3.27e-05 | ms/batch 104.28 | loss  2.24 | avg loss  2.49 | ppl 12.10\n",
            "| epoch   2 step    35300 |  14269 batches | lr 3.22e-05 | ms/batch 103.92 | loss  2.33 | avg loss  2.59 | ppl 13.33\n",
            "| epoch   2 step    35400 |  14369 batches | lr 3.18e-05 | ms/batch 104.21 | loss  2.02 | avg loss  2.58 | ppl 13.26\n",
            "| epoch   2 step    35500 |  14469 batches | lr 3.13e-05 | ms/batch 104.24 | loss  3.14 | avg loss  2.51 | ppl 12.31\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.35500.pt\n",
            "| epoch   2 step    35600 |  14569 batches | lr 3.08e-05 | ms/batch 104.49 | loss  2.69 | avg loss  2.54 | ppl 12.71\n",
            "| epoch   2 step    35700 |  14669 batches | lr 3.03e-05 | ms/batch 104.24 | loss  3.22 | avg loss  2.60 | ppl 13.49\n",
            "| epoch   2 step    35800 |  14769 batches | lr 2.98e-05 | ms/batch 104.22 | loss  2.89 | avg loss  2.56 | ppl 12.97\n",
            "| epoch   2 step    35900 |  14869 batches | lr 2.94e-05 | ms/batch 104.05 | loss  2.89 | avg loss  2.49 | ppl 12.06\n",
            "| epoch   2 step    36000 |  14969 batches | lr 2.89e-05 | ms/batch 104.11 | loss  2.32 | avg loss  2.53 | ppl 12.59\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.36000.pt\n",
            "eval samples: 0 loss: tensor(1.0650, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5050, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8212, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0584, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9402, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0693, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3836, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0109, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4532, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9779, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9817, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0694, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9823, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8649, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1458, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0609, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2895, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.5935, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2623, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3611, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.1742, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5881, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3089, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3664, device='cuda:0')\n",
            "average loss 1.2202928929321775\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  18 at step    36000 | time: 98.76s | valid loss  1.22 | valid ppl  3.39 | best ppl  3.39 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    36100 |  15069 batches | lr 2.84e-05 | ms/batch 1092.09 | loss  2.46 | avg loss  2.51 | ppl 12.26\n",
            "| epoch   2 step    36200 |  15169 batches | lr 2.79e-05 | ms/batch 104.25 | loss  2.25 | avg loss  2.55 | ppl 12.74\n",
            "| epoch   2 step    36300 |  15269 batches | lr 2.75e-05 | ms/batch 104.45 | loss  1.95 | avg loss  2.55 | ppl 12.85\n",
            "| epoch   2 step    36400 |  15369 batches | lr 2.7e-05 | ms/batch 104.37 | loss  2.91 | avg loss  2.52 | ppl 12.47\n",
            "| epoch   2 step    36500 |  15469 batches | lr 2.65e-05 | ms/batch 104.17 | loss  2.67 | avg loss  2.56 | ppl 12.99\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.36500.pt\n",
            "| epoch   2 step    36600 |  15569 batches | lr 2.6e-05 | ms/batch 104.26 | loss  2.59 | avg loss  2.59 | ppl 13.28\n",
            "| epoch   2 step    36700 |  15669 batches | lr 2.56e-05 | ms/batch 103.98 | loss  2.44 | avg loss  2.49 | ppl 12.04\n",
            "| epoch   2 step    36800 |  15769 batches | lr 2.51e-05 | ms/batch 104.02 | loss  2.54 | avg loss  2.57 | ppl 13.04\n",
            "| epoch   2 step    36900 |  15869 batches | lr 2.46e-05 | ms/batch 104.03 | loss  2.60 | avg loss  2.60 | ppl 13.44\n",
            "| epoch   2 step    37000 |  15969 batches | lr 2.41e-05 | ms/batch 104.09 | loss  3.15 | avg loss  2.54 | ppl 12.66\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.37000.pt\n",
            "| epoch   2 step    37100 |  16069 batches | lr 2.36e-05 | ms/batch 104.09 | loss  2.33 | avg loss  2.56 | ppl 12.87\n",
            "| epoch   2 step    37200 |  16169 batches | lr 2.32e-05 | ms/batch 103.84 | loss  2.69 | avg loss  2.58 | ppl 13.14\n",
            "| epoch   2 step    37300 |  16269 batches | lr 2.27e-05 | ms/batch 103.87 | loss  2.83 | avg loss  2.56 | ppl 12.96\n",
            "| epoch   2 step    37400 |  16369 batches | lr 2.22e-05 | ms/batch 104.18 | loss  2.46 | avg loss  2.51 | ppl 12.28\n",
            "| epoch   2 step    37500 |  16469 batches | lr 2.17e-05 | ms/batch 103.97 | loss  2.49 | avg loss  2.57 | ppl 13.04\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.37500.pt\n",
            "| epoch   2 step    37600 |  16569 batches | lr 2.13e-05 | ms/batch 103.89 | loss  2.41 | avg loss  2.49 | ppl 12.04\n",
            "| epoch   2 step    37700 |  16669 batches | lr 2.08e-05 | ms/batch 103.96 | loss  2.43 | avg loss  2.60 | ppl 13.40\n",
            "| epoch   2 step    37800 |  16769 batches | lr 2.03e-05 | ms/batch 103.96 | loss  3.17 | avg loss  2.60 | ppl 13.50\n",
            "| epoch   2 step    37900 |  16869 batches | lr 1.98e-05 | ms/batch 103.76 | loss  2.60 | avg loss  2.52 | ppl 12.41\n",
            "| epoch   2 step    38000 |  16969 batches | lr 1.94e-05 | ms/batch 103.78 | loss  2.54 | avg loss  2.56 | ppl 12.94\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.38000.pt\n",
            "eval samples: 0 loss: tensor(1.0818, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5110, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8480, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0381, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9626, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0835, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3875, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.9992, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4605, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9641, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9806, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0547, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9622, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8486, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1499, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0566, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2737, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6064, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2425, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3686, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.1835, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.5818, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3130, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3581, device='cuda:0')\n",
            "average loss 1.2202040423728424\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  19 at step    38000 | time: 98.63s | valid loss  1.22 | valid ppl  3.39 | best ppl  3.39 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    38100 |  17069 batches | lr 1.89e-05 | ms/batch 1090.07 | loss  2.23 | avg loss  2.57 | ppl 13.01\n",
            "| epoch   2 step    38200 |  17169 batches | lr 1.84e-05 | ms/batch 103.77 | loss  2.32 | avg loss  2.51 | ppl 12.36\n",
            "| epoch   2 step    38300 |  17269 batches | lr 1.79e-05 | ms/batch 103.79 | loss  2.72 | avg loss  2.56 | ppl 12.93\n",
            "| epoch   2 step    38400 |  17369 batches | lr 1.75e-05 | ms/batch 103.80 | loss  2.33 | avg loss  2.53 | ppl 12.55\n",
            "| epoch   2 step    38500 |  17469 batches | lr 1.7e-05 | ms/batch 103.71 | loss  2.40 | avg loss  2.55 | ppl 12.81\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.38500.pt\n",
            "| epoch   2 step    38600 |  17569 batches | lr 1.65e-05 | ms/batch 104.07 | loss  2.59 | avg loss  2.58 | ppl 13.19\n",
            "| epoch   2 step    38700 |  17669 batches | lr 1.6e-05 | ms/batch 104.05 | loss  2.25 | avg loss  2.58 | ppl 13.16\n",
            "| epoch   2 step    38800 |  17769 batches | lr 1.55e-05 | ms/batch 104.15 | loss  2.95 | avg loss  2.58 | ppl 13.15\n",
            "| epoch   2 step    38900 |  17869 batches | lr 1.51e-05 | ms/batch 104.45 | loss  2.75 | avg loss  2.51 | ppl 12.36\n",
            "| epoch   2 step    39000 |  17969 batches | lr 1.46e-05 | ms/batch 104.43 | loss  2.79 | avg loss  2.54 | ppl 12.64\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.39000.pt\n",
            "| epoch   2 step    39100 |  18069 batches | lr 1.41e-05 | ms/batch 104.53 | loss  3.78 | avg loss  2.58 | ppl 13.20\n",
            "| epoch   2 step    39200 |  18169 batches | lr 1.36e-05 | ms/batch 104.07 | loss  2.26 | avg loss  2.55 | ppl 12.80\n",
            "| epoch   2 step    39300 |  18269 batches | lr 1.32e-05 | ms/batch 103.94 | loss  3.62 | avg loss  2.57 | ppl 13.06\n",
            "| epoch   2 step    39400 |  18369 batches | lr 1.27e-05 | ms/batch 103.93 | loss  2.48 | avg loss  2.53 | ppl 12.57\n",
            "| epoch   2 step    39500 |  18469 batches | lr 1.22e-05 | ms/batch 103.87 | loss  2.26 | avg loss  2.53 | ppl 12.52\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.39500.pt\n",
            "| epoch   2 step    39600 |  18569 batches | lr 1.17e-05 | ms/batch 104.21 | loss  2.89 | avg loss  2.56 | ppl 12.97\n",
            "| epoch   2 step    39700 |  18669 batches | lr 1.13e-05 | ms/batch 104.08 | loss  3.81 | avg loss  2.53 | ppl 12.61\n",
            "| epoch   2 step    39800 |  18769 batches | lr 1.08e-05 | ms/batch 103.91 | loss  2.50 | avg loss  2.60 | ppl 13.42\n",
            "| epoch   2 step    39900 |  18869 batches | lr 1.03e-05 | ms/batch 103.81 | loss  2.49 | avg loss  2.59 | ppl 13.36\n",
            "| epoch   2 step    40000 |  18969 batches | lr 9.83e-06 | ms/batch 104.02 | loss  3.04 | avg loss  2.59 | ppl 13.29\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.40000.pt\n",
            "eval samples: 0 loss: tensor(1.0723, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5199, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8276, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0379, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9593, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0832, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3918, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.9967, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4634, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9833, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9998, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0716, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9566, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8377, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1653, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0594, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2818, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.6037, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2365, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3538, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.1820, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6088, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3116, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3636, device='cuda:0')\n",
            "average loss 1.218340642597169\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  20 at step    40000 | time: 98.68s | valid loss  1.22 | valid ppl  3.38 | best ppl  3.38 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    40100 |  19069 batches | lr 9.35e-06 | ms/batch 1091.21 | loss  2.33 | avg loss  2.52 | ppl 12.46\n",
            "| epoch   2 step    40200 |  19169 batches | lr 8.87e-06 | ms/batch 104.42 | loss  2.27 | avg loss  2.58 | ppl 13.23\n",
            "| epoch   2 step    40300 |  19269 batches | lr 8.4e-06 | ms/batch 104.22 | loss  2.90 | avg loss  2.48 | ppl 11.97\n",
            "| epoch   2 step    40400 |  19369 batches | lr 7.92e-06 | ms/batch 104.17 | loss  2.87 | avg loss  2.58 | ppl 13.21\n",
            "| epoch   2 step    40500 |  19469 batches | lr 7.44e-06 | ms/batch 103.81 | loss  2.34 | avg loss  2.58 | ppl 13.22\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.40500.pt\n",
            "| epoch   2 step    40600 |  19569 batches | lr 6.97e-06 | ms/batch 104.30 | loss  1.94 | avg loss  2.53 | ppl 12.57\n",
            "| epoch   2 step    40700 |  19669 batches | lr 6.49e-06 | ms/batch 104.39 | loss  2.52 | avg loss  2.50 | ppl 12.19\n",
            "| epoch   2 step    40800 |  19769 batches | lr 6.01e-06 | ms/batch 104.36 | loss  2.27 | avg loss  2.61 | ppl 13.64\n",
            "| epoch   2 step    40900 |  19869 batches | lr 5.54e-06 | ms/batch 104.08 | loss  2.36 | avg loss  2.58 | ppl 13.26\n",
            "| epoch   2 step    41000 |  19969 batches | lr 5.06e-06 | ms/batch 103.93 | loss  2.60 | avg loss  2.53 | ppl 12.59\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.41000.pt\n",
            "| epoch   2 step    41100 |  20069 batches | lr 4.59e-06 | ms/batch 104.16 | loss  2.16 | avg loss  2.55 | ppl 12.86\n",
            "| epoch   2 step    41200 |  20169 batches | lr 4.11e-06 | ms/batch 103.99 | loss  2.99 | avg loss  2.57 | ppl 13.09\n",
            "| epoch   2 step    41300 |  20269 batches | lr 3.63e-06 | ms/batch 104.05 | loss  2.18 | avg loss  2.54 | ppl 12.69\n",
            "| epoch   2 step    41400 |  20369 batches | lr 3.16e-06 | ms/batch 104.22 | loss  2.88 | avg loss  2.56 | ppl 12.94\n",
            "| epoch   2 step    41500 |  20469 batches | lr 2.68e-06 | ms/batch 104.08 | loss  2.68 | avg loss  2.54 | ppl 12.67\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.41500.pt\n",
            "| epoch   2 step    41600 |  20569 batches | lr 2.2e-06 | ms/batch 104.03 | loss  2.51 | avg loss  2.58 | ppl 13.15\n",
            "| epoch   2 step    41700 |  20669 batches | lr 1.73e-06 | ms/batch 104.19 | loss  2.62 | avg loss  2.56 | ppl 12.90\n",
            "| epoch   2 step    41800 |  20769 batches | lr 1.25e-06 | ms/batch 104.36 | loss  2.12 | avg loss  2.50 | ppl 12.16\n",
            "| epoch   2 step    41900 |  20869 batches | lr 7.72e-07 | ms/batch 103.88 | loss  2.10 | avg loss  2.54 | ppl 12.64\n",
            "| epoch   2 step    42000 |  20969 batches | lr 2.96e-07 | ms/batch 104.11 | loss  2.15 | avg loss  2.59 | ppl 13.28\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.42000.pt\n",
            "eval samples: 0 loss: tensor(1.0678, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.5072, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(0.8255, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0309, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9562, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(1.0780, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3935, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(1.0046, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.4615, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(0.9745, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.9918, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.0692, device='cuda:0')\n",
            "eval samples: 1200 loss: tensor(0.9653, device='cuda:0')\n",
            "eval samples: 1300 loss: tensor(0.8321, device='cuda:0')\n",
            "eval samples: 1400 loss: tensor(1.1470, device='cuda:0')\n",
            "eval samples: 1500 loss: tensor(1.0623, device='cuda:0')\n",
            "eval samples: 1600 loss: tensor(1.2695, device='cuda:0')\n",
            "eval samples: 1700 loss: tensor(0.5977, device='cuda:0')\n",
            "eval samples: 1800 loss: tensor(1.2368, device='cuda:0')\n",
            "eval samples: 1900 loss: tensor(1.3553, device='cuda:0')\n",
            "eval samples: 2000 loss: tensor(1.1794, device='cuda:0')\n",
            "eval samples: 2100 loss: tensor(1.6009, device='cuda:0')\n",
            "eval samples: 2200 loss: tensor(1.3127, device='cuda:0')\n",
            "eval samples: 2300 loss: tensor(1.3535, device='cuda:0')\n",
            "average loss 1.2160825027943882\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  21 at step    42000 | time: 98.76s | valid loss  1.22 | valid ppl  3.37 | best ppl  3.37 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.42062.pt\n",
            "----------------------------------------------------------------------------------------------------\n",
            "End of training\n",
            "cleanup dist ...\n",
            "✓ Training with reduced batch size completed!\n"
          ]
        }
      ]
    }
  ]
}